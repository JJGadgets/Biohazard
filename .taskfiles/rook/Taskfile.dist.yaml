---
version: "3"

x-task-vars: &task-vars
  NODE: "{{.NODE}}"
  CEPH_DISK: "{{.CEPH_DISK}}"
  TIME: "{{.TIME}}"
  JOB_NAME: "{{.JOB_NAME}}"

vars:
  TIME: '{{now | date "150405"}}'

includes:
  k8s:
    internal: true
    taskfile: ../k8s
  cluster:
    internal: true
    taskfile: ../cluster
  talos:
    internal: true
    taskfile: ../talos

tasks:
  osd-prepare-logs:
    aliases: [osdlogs]
    desc: Stream all logs for the `osd-prepare` Job.
    cmds:
      - while true; do kubectl logs -n rook-ceph -l app=rook-ceph-osd-prepare -c provision --tail 999999999999999999 -f 2>&1 | grep -v "No resources found" | tee --append /tmp/rook-ceph-osd-prepare-{{- .TIME -}}.log; done
      # - |-
      #   JOBDONE=no
      #   until $JOBDONE == "yes"; do
      #     kubectl logs -n rook-ceph jobs/rook-ceph-osd-prepare-blackfish --tail 999999999999999999 -f &
      #     PID1=$!
      #     kubectl wait -n rook-ceph job -l app=rook-ceph-osd-prepare --for condition=complete --timeout=1m && kill $PID1 && JOBDONE=yes
      #   done
      # - |-
      #   kubectl logs -n rook-ceph -l app=rook-ceph-osd-prepare --tail 999999999999999999 -f;
      #   until kubectl wait -n rook-ceph job -l app=rook-ceph-osd-prepare --for condition=complete --timeout=1m; do
      #     kubectl logs -n rook-ceph -l app=rook-ceph-osd-prepare --tail 999999999999999999 -f | tee /tmp/rook-ceph-osd-prepare.log;
      #   done

  osd-prepare-copy-job-debug:
    aliases: [osdjobcp]
    desc: Copy OSD-prepare job YAML, remove unnecessary fields, change command ran for user to manually shell and run commands. Used for debugging OSD-prepare job with its env and configurations remaining.
    silent: true
    vars:
      NODE: '{{ or .NODE (fail "`NODE` is required") }}'
    cmds:
      - |-
        kubectl get jobs -n rook-ceph rook-ceph-osd-prepare-{{.NODE}} -o yaml | yq 'del(.status, .metadata.annotations, .metadata.creationTimestamp, .metadata.generation, .metadata.ownerReferences, .metadata.resourceVersion, .metadata.uid, .spec.selector, .spec.template.metadata.labels.controller-uid) | .metadata.name = "test-rook-ceph-osd-prepare-{{.NODE}}" | .spec.template.metadata.labels.job-name = "test-rook-ceph-osd-prepare-{{.NODE}}" | .spec.template.spec.containers.0.command = ["/bin/sh", "-c"] | .spec.template.spec.containers.0.args = ["sleep 999999999"]' | kubectl apply -f -

  zap-disk:
    desc: Prepare a disk to be used as a Ceph OSD on specified node by zapping all data and partition data.
    dir: '/{{.ROOT_DIR}}/.taskfiles/rook'
    cmds:
      # TODO: mimic deviceFilter
      - envsubst < <(cat {{.JOB_TEMPLATE}}) | kubectl apply -f -
      - |- 
        kubectl -n kube-system logs job/{{.JOB_NAME}} -f || true;
        until kubectl -n kube-system wait job/{{.JOB_NAME}} --for condition=complete --timeout=2s; do 
          echo "Job {{.JOB_NAME}} is still running, logs:" && 
          kubectl -n kube-system logs job/{{.JOB_NAME}} -f;
        done;
      # - kubectl -n kube-system logs job/{{.JOB_NAME}}
      - defer: kubectl -n kube-system delete job {{.JOB_NAME}}
    vars:
      NODE: '{{ or .NODE (fail "`NODE` is required") }}'
      CEPH_DISK: '{{ or .CEPH_DISK (fail "`CEPH_DISK` is required") }}'
      JOB_NAME: 'zap-disk-{{- .NODE -}}-{{- .TIME -}}'
      JOB_TEMPLATE: "zap-disk-job.tmpl.yaml"
    env: *task-vars
    preconditions:
      - sh: test -f {{.JOB_TEMPLATE}}

  wipe-state:
    desc: Wipe all Ceph state on specified node.
    dir: '/{{.ROOT_DIR}}/.taskfiles/rook'
    cmds:
      - envsubst < <(cat {{.JOB_TEMPLATE}}) | kubectl apply -f -
      - until kubectl -n kube-system wait job/{{.JOB_NAME}} --for condition=complete --timeout=2s; do echo "Job {{.JOB_NAME}} is still running, logs:" && kubectl -n kube-system logs job/{{.JOB_NAME}} -f; done
      - defer: kubectl -n kube-system delete job {{.JOB_NAME}}
    vars:
      NODE: '{{ or .NODE (fail "`NODE` is required") }}'
      JOB_NAME: "wipe-rook-state-{{- .NODE -}}-{{- .TIME -}}"
      JOB_TEMPLATE: "wipe-rook-state-job.tmpl.yaml"
    env: *task-vars
    preconditions:
      - sh: test -f {{.JOB_TEMPLATE}}

  wipe-node:
    aliases: ["wn"]
    desc: Trigger a wipe of all Rook-Ceph data on specified node.
    dir: '/{{.ROOT_DIR}}/.taskfiles/rook'
    vars:
      NODE: '{{ or .NODE (fail "Missing `NODE` environment variable!") }}'
      CEPH_DISK: '{{ or .CEPH_DISK (fail "Missing `CEPH_DISK` environment variable!") }}'
    cmds:
      - task: zap-disk
        vars:
          NODE: '{{.NODE}}'
          CEPH_DISK: '{{ or .CEPH_DISK (fail "Missing `CEPH_DISK` environment variable!") }}'
      - task: wipe-state
        vars:
          NODE: '{{.NODE}}'
      - task: talos:reboot
        vars:
          NODE: '{{.NODE}}'

  wipe-nodes-nuclear:
    desc: Wipe all nodes in cluster "nuclear"
    dir: '/{{.ROOT_DIR}}/.taskfiles/rook'
    cmds:
      - task: wipe-node
        vars:
          NODE: "blackfish"
          CEPH_DISK: "/dev/disk/by-id/ata-INTEL_SSDSC2BB016T4_BTWD709202JK1P6HGN" # I swear I'll forget to update this here when I have to change the disk
      - task: wipe-node
        vars:
          NODE: "humming"
          CEPH_DISK: "/dev/disk/by-id/ata-INTEL_SSDSC2BB016T4_BTWD709202L91P6HGN"

  reinstall:
    desc: |-
      For when Rook refuses to create any OSDs at all
      Assuming Flux and resource names, suspends master ks.yaml (Flux Kustomization), suspends ks.yaml for Rook-Ceph and cluster, suspends HelmReleases for Rook-Ceph and cluster, deletes cluster HelmRelease, patches Ceph CR and cm/secret finalizers, removes Rook-Ceph HR and namespace.
      Then, reconcile master, Rook-Ceph and cluster ks.yaml.
    dir: '/{{.ROOT_DIR}}/.taskfiles/rook'
    vars:
      C: '{{ or .C (fail "Missing `C` environment variable for cluster!") }}'
    cmds:
      - task: cluster:cluster-switch
        vars:
          C: '{{.C}}'
      - flux suspend ks 0-{{.C}}-config
      - flux suspend ks 1-core-storage-rook-ceph-app
      - flux suspend ks 1-core-storage-rook-ceph-cluster
      - helm uninstall -n rook-ceph rook-ceph-cluster && true || true
      - flux delete hr -n rook-ceph rook-ceph-cluster --silent && true || true
      - |-
        for CRD in $(kubectl get crd -n rook-ceph | awk '/ceph.rook.io/ {print $1}'); do
            kubectl get -n rook-ceph "$CRD" -o name | \
            xargs -I {} kubectl patch -n rook-ceph {} --type merge -p '{"metadata":{"finalizers": []}}' && true || true
        done
      - |-
        kubectl -n rook-ceph patch configmap rook-ceph-mon-endpoints --type merge -p '{"metadata":{"finalizers": []}}' && true || true
        kubectl -n rook-ceph patch secrets rook-ceph-mon --type merge -p '{"metadata":{"finalizers": []}}' && true || true
      - helm uninstall -n rook-ceph rook-ceph && true || true
      - flux delete hr -n rook-ceph rook-ceph --silent && true || true
      - kubectl get namespaces rook-ceph && until kubectl delete namespaces rook-ceph; do kubectl get namespaces rook-ceph -o jsonpath="{.status}"; done || true
      - task: wipe-nodes-{{.C}}
      - task: wipe-nodes-{{.C}} # run again in case wipe fails
      - flux suspend ks 0-{{.C}}-config && flux resume ks 0-{{.C}}-config
      - flux suspend ks 1-core-storage-rook-ceph-app && flux resume ks 1-core-storage-rook-ceph-app
      - flux suspend ks 1-core-storage-rook-ceph-cluster && flux resume ks 1-core-storage-rook-ceph-cluster
      # - task: osd-prepare-logs # uncomment to debug OSD prepare errors with while true loop, since the job re-runs 6 times and overwrites the previous runs' logs