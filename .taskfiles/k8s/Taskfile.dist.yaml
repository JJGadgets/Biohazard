---
# General Kubernetes admin tasks
version: "3"

vars:
  # C: '{{ .C | default "admin@biohazard" }}'
  TIMENOW:
    sh: date +%Y%m%d-%H%M%S

tasks:
  race-ns-pod-security:
    aliases: [nsps]
    desc: While true loop labelling a given namespace with Pod Security labels, if external source creates namespace (e.g. Azure Arc's Helm apply with `--create-namespace`)
    vars:
      NS: &ns-fail '{{ or .NS (fail "Missing `NS` environment variable!") }}'
      PS: '{{ or .PS (fail "Missing `PS` environment variable!") }}'
    cmds:
      - while true; do kubectl label namespaces {{.NS}} "pod-security.kubernetes.io/enforce={{.PS}}" >/dev/null 2>/dev/null || true; kubectl label namespaces {{.NS}} "pod-security.kubernetes.io/enforce-version=latest" >/dev/null 2>/dev/null || true; done

  wait-pod-pending:
    aliases: [waitp]
    internal: true
    desc: Wait for a job's pod to change its status to pending
    vars:
      NAME: &name-fail '{{ or .NAME (fail "Missing `NAME` environment variable!") }}'
      NS: *ns-fail
    cmds:
      - |
        until [[ $(kubectl -n {{.NS}} get pod {{.NAME}} -o jsonpath='{.items[*].status.phase}') == "Pending" ]]; do sleep 1; done

  wait-pod-running:
    aliases: [waitr]
    internal: true
    desc: "Wait for a job's pod to change its status to running"
    vars:
      NAME: *name-fail
      NS: *ns-fail
    cmds:
      - until [[ $(kubectl -n {{.NS}} get pod {{.NAME}} -o jsonpath='{.items[*].status.phase}') == "Running" ]]; do sleep 1; done

  wait-pod-running-v2:
    internal: true
    desc: Wait for a job's pod to change its status to running
    vars:
      NAME: *name-fail
      NS: *ns-fail
    cmds:
      - |
        until kubectl wait pod -n {{.NS}} {{.NAME}} --for-jsonpath='{.items[*].status.phase}'=Running --timeout=-; do sleep 1; done

  wait-pod-ready:
    internal: true
    desc: Wait for a pod to be ready
    vars:
      NAME: *name-fail
      NS: *ns-fail
    cmds:
      - until kubectl wait pod -n {{.NS}} {{.NAME}} --for=condition=Ready --timeout=1h; do sleep 1; done

  wait-pod-delete:
    aliases: [waitd]
    internal: true
    desc: Wait for a job's pod to delete
    vars:
      NAME: *name-fail
      NS: *ns-fail
    cmds:
      - until kubectl wait pod -n {{.NS}} {{.NAME}} --for delete --timeout=1m; do sleep 1; done

  wait-finish:
    internal: true
    desc: Wait for a job's pod to change its status to pending
    vars:
      NAME: *name-fail
      NS: *ns-fail
      TYPE: '{{ .TYPE | default "job" }}'
      # WAIT_ARGS: '{{.WAIT_ARGS | default "echo \"{{.NAME}} is still running, logs:\" && kubectl -n {{.NS}} logs {{.NAME}} --since 2s -f;"}}'
    cmds:
      - |-
        until kubectl -n {{.NS}} wait {{.TYPE}}/{{.NAME}} --for condition=complete --timeout=2s; do
          echo "{{.NAME}} is still running, logs:" && kubectl -n {{.NS}} logs {{.TYPE}}/{{.NAME}} --since 2s -f || true;
        done

  get-public-ingresses:
    aliases: [gpi]
    desc: |
      TL;dr: Check which ingress resources are exposed publicly via DNS to specific target.
      Long Description: Search through all ingress resources for names of which resources have specific external-dns annotation values.
    vars:
      dns: '{{ or .dns (fail "Missing search query!") }}'
    cmds:
      - kubectl get ingress,svc --all-namespaces=true -o yaml | yq -r '.items | map(select(.metadata.annotations."external-dns.alpha.kubernetes.io/target"=="*{{.dns}}*").metadata.name)'

  sops-apply:
    aliases: [sa]
    desc: Decrypt cluster secrets and vars to envsubst local resource, then apply substituted resource.
    vars:
      F: '{{ or .F (fail "Missing file (`f` var) to `envsubst && k apply`!") }}'
      SECRETS_FILE: "{{.ROOT_DIR}}/kube/clusters/{{.C}}/config/secrets.sops.env"
      VARS_FILE: "{{.ROOT_DIR}}/kube/clusters/{{.C}}/config/vars.sops.env"
      FCMD: '{{ .FCMD | default "cat" }}'
      C: '{{.C | default "admin@biohazard"}}'
    cmds:
      #- sops exec-env {{.SECRETS_FILE}} "sops exec-env {{.VARS_FILE}} \"{{.FCMD}} {{.F}} | envsubst | kubectl apply --context {{.C}} -f -\""
      - sops exec-env {{.SECRETS_FILE}} "sops exec-env {{.VARS_FILE}} \"{{.FCMD}} {{.F}} | envsubst | kubectl apply -f -\""

  cilium-bootstrap-apply:
    aliases: [cilium]
    desc: Bootstrap Cilium onto new cluster, ready for Flux to take over managing Cilium.
    # dir: "/{{.ROOT_DIR}}/kube/clusters/{{.C}}/talos"
    vars:
      C: '{{ or .C (fail "Missing C environment variable for cluster!") }}'
    cmds:
      - kubectl delete configmap -n kube-system cilium-config || true
      - kubectl delete daemonset -n kube-system cilium || true
      - kubectl delete deployment -n kube-system cilium-operator || true
      #- cp /{{.ROOT_DIR}}/kube/deploy/core/_networking/cilium/clusters/{{.C}}/helm-values.yaml /{{.ROOT_DIR}}/kube/deploy/core/_networking/cilium/app/bootstrap-install/base-values.yaml
      - task: sops-apply
        vars:
          C: '{{.C | default "admin@biohazard"}}'
          F: "/{{.ROOT_DIR}}/kube/deploy/core/_networking/cilium/clusters/{{.C}}/"
          FCMD: "kustomize build --enable-helm"
      #- defer: rm -rf /{{.ROOT_DIR}}/kube/deploy/core/_networking/cilium/app/bootstrap-install/base-values.yaml

  newapp:
    desc: Copy app folder structure template, substitute APPNAME, and (TODO) prompt user for variables values such as DNS, UID etc.
    vars:
      APP: &app-fail '{{ or .APP (fail "Missing `app` variable!") }}'
      IMAGENAME: &imagename-fail '{{ or .IMAGENAME (fail "Missing `IMAGENAME` variable!") }}'
      IMAGETAG: &imagetag-fail '{{ or .IMAGETAG (fail "Missing `IMAGETAG` variable!") }}'
    cmds:
      - cp -r ./kube/templates/test ./kube/deploy/apps/{{.APP}}
      # lowercase, used for resource names etc
      - grep -lR 'APPNAME' ./kube/deploy/apps/{{.APP}}/ | xargs -I% sed -i 's/${APPNAME}/{{.APP}}/g' %
      - grep -lR 'IMAGENAME' ./kube/deploy/apps/{{.APP}}/ | xargs -I% sed -i 's/${IMAGENAME}/{{.IMAGENAME}}/g' %
      - grep -lR 'IMAGETAG' ./kube/deploy/apps/{{.APP}}/ | xargs -I% sed -i 's/${IMAGETAG}/{{.IMAGETAG}}/g' %
      # uppercase, for variable substitution references e.g. ${APP_DNS_AUTHENTIK}
      - grep -lR 'APPNAME' ./kube/deploy/apps/{{.APP}}/ | xargs -I% sed -i 's/_APPNAME:=/_{{.APP}}:=/g;s/\(_{{.APP}}:=\)/\U\1/g' %
      - grep -lR 'APPNAME' ./kube/deploy/apps/{{.APP}}/ | xargs -I% sed -i 's/_APPNAME}/_{{.APP}}}/g;s/\(_{{.APP}}}\)/\U\1/g' %

  shortnames:
    desc: List all installed CRDs and their short names.
    cmds:
      - |
        kubectl get crds -o jsonpath '{range .items[*]}{.spec.names.kind}: {.spec.names.shortNames}{"\n"}{end}'

  clear-old-pods:
    aliases: [cop]
    desc: Clears all old pods remaining after a reboot
    cmds:
      - |-
        while true; do
          kubectl delete pod --wait=false --all-namespaces=true --field-selector=status.phase==Failed || true;
          kubectl delete pod --wait=false --all-namespaces=true --field-selector=status.phase==Pending || true;
          kubectl delete pod --wait=false --all-namespaces=true --field-selector=status.phase==Succeeded || true;
        done

  delete-stuck-pvc:
    aliases: [delpvc]
    desc: Delete PVC which is stuck, e.g. if it's a local-path PVC and the node has been wiped and reset.
    vars:
      #NS: '{{.NS | default "default"}}'
      NS: '{{ or .NS (fail "Missing `NS` environment variable!") }}'
      PVC: '{{ or .PVC (fail "Missing `PVC` environment variable!") }}'
    cmds:
      #- |
      #  kubectl get pvc -n {{.NS}} {{.PVC}} -o jsonpath='{.spec.volumeName}'
      - |
        kubectl delete pvc -n {{.NS}} {{.PVC}} --wait=false
      - |
        kubectl patch pvc -n {{.NS}} {{.PVC}} --type='json' -p '[{"op": "remove", "path": "/metadata/finalizers"}]'
      - |
        until kubectl wait --for=delete pvc -n {{.NS}} {{.PVC}} --timeout=3600s; do sleep 1; done

  delete-all-reset-node-pvcs:
    desc: Delete all PVCs that are stuck because they're tied to a node that has been reset.
    vars:
      NODE: '{{ or .NODE (fail "Missing `NODE` environment variable!") }}'
      FAILCONTINUE: '{{ .FAILCONTINUE | default "false" }}'
    cmds:
      - |
        kubectl get pv -o jsonpath='{range .items[?(@.spec.nodeAffinity.required.nodeSelectorTerms[*].matchExpressions[*].values[*]=="{{.NODE}}")]}{"NS="}{.spec.claimRef.namespace}{" "}{"PVC="}{.spec.claimRef.name}{"\n"}{end}' | while read -r i; do
          task k8s:delete-stuck-pvc ${i} || {{.FAILCONTINUE}}
        done

  iperf2:
    desc: Start a iperf2 server on one node, and iperf2 client on another node, to benchmark network performance.
    dir: "/{{.ROOT_DIR}}/.taskfiles/k8s/template/iperf2"
    vars: &iperf2-vars
      SERVER_NAME: &iperf2-server-name 'iperf2-server-{{- .TIMENOW -}}'
      SERVER_NS: &iperf2-server-ns '{{ .SERVER_NS | default "default" }}'
      CLIENT_NAME: &iperf2-client-name 'iperf2-client-{{- .TIMENOW -}}'
      CLIENT_NS: &iperf2-client-ns '{{ .CLIENT_NS | default "default" }}'
      CLUSTER_DOMAIN: '{{ .CLUSTER_DOMAIN | default "cluster.local" }}'
      SERVER_PORT: '{{ .SERVER_PORT | default "5001" }}'
      SERVER_NODE: '{{ or .SERVER_NODE (fail "Missing `SERVER_NODE` environment variable!") }}'
      CLIENT_NODE: '{{ or .CLIENT_NODE (fail "Missing `CLIENT_NODE` environment variable!") }}'
      SERVER_ARGS: '{{ .SERVER_ARGS | default "" }}'
      CLIENT_ARGS: '{{ .CLIENT_ARGS | default "" }}'
    env: *iperf2-vars
    cmds:
      - cat ./server.yaml | envsubst | kubectl apply -f -
      - defer: cat ./server.yaml | envsubst | kubectl delete -f -
      - task: wait-pod-ready
        vars:
          NAME: '-l job-name={{.SERVER_NAME}}'
          NS: '{{.SERVER_NS}}'
      - cat ./client.yaml | envsubst | kubectl apply -f -
      - defer: cat ./client.yaml | envsubst | kubectl delete -f -
      - task: wait-finish
        vars:
          NAME: '{{.CLIENT_NAME}}'
          NS: '{{.CLIENT_NS}}'

  kbench:
    vars: &kbench-vars
      # SC: '{{ or .SC (fail "Missing `SC` environment variable!") }}'
      SC: '{{.SC}}'
      NODE: '{{.NODE}}'
      NS: '{{ .NS | default "default" }}'
    env: *kbench-vars
    cmds:
      - &kbench-delete |-
        export KBENCH=$(curl -sL https://raw.githubusercontent.com/yasker/kbench/main/deploy/fio.yaml)
        [[ ! -z "{{.SC}}" ]] && export KBENCH=$(printf '%s\n' "${KBENCH}" | yq '. | select(.kind == "PersistentVolumeClaim").spec.storageClassName = "{{.SC}}"')
        [[ ! -z "{{.NODE}}" ]] && export KBENCH=$(printf '%s\n' "${KBENCH}" | yq '. | select(.kind == "Job").spec.template.spec.nodeSelector."kubernetes.io/hostname" = "{{.NODE}}"')
        export KBENCH=$(printf '%s\n' "${KBENCH}" | yq '. | select(.kind == "PersistentVolumeClaim").metadata.name = "kbench-{{- .TIMENOW -}}"')
        export KBENCH=$(printf '%s\n' "${KBENCH}" | yq '. | select(.kind == "Job").metadata.name = "kbench-{{- .TIMENOW -}}"')
        export KBENCH=$(printf '%s\n' "${KBENCH}" | yq '. | (select(.kind == "Job").spec.template.spec.volumes.[] | select(.name == "vol").persistentVolumeClaim.claimName) |= "kbench-{{- .TIMENOW -}}"')
        printf '%s\n' "${KBENCH}" | kubectl delete -n {{.NS}} -f - || true
      - |-
        export KBENCH=$(curl -sL https://raw.githubusercontent.com/yasker/kbench/main/deploy/fio.yaml)
        [[ ! -z "{{.SC}}" ]] && export KBENCH=$(printf '%s\n' "${KBENCH}" | yq '. | select(.kind == "PersistentVolumeClaim").spec.storageClassName = "{{.SC}}"')
        [[ ! -z "{{.NODE}}" ]] && export KBENCH=$(printf '%s\n' "${KBENCH}" | yq '. | select(.kind == "Job").spec.template.spec.nodeSelector."kubernetes.io/hostname" = "{{.NODE}}"')
        export KBENCH=$(printf '%s\n' "${KBENCH}" | yq '. | select(.kind == "PersistentVolumeClaim").metadata.name = "kbench-{{- .TIMENOW -}}"')
        export KBENCH=$(printf '%s\n' "${KBENCH}" | yq '. | select(.kind == "Job").metadata.name = "kbench-{{- .TIMENOW -}}"')
        export KBENCH=$(printf '%s\n' "${KBENCH}" | yq '. | (select(.kind == "Job").spec.template.spec.volumes.[] | select(.name == "vol").persistentVolumeClaim.claimName) |= "kbench-{{- .TIMENOW -}}"')
        printf '%s\n' "${KBENCH}" | kubectl apply -n {{.NS}} -f -
      - defer: *kbench-delete
      - task: wait-finish
        vars:
          NS: '{{ .NS | default "default" }}'
          NAME: "kbench-{{- .TIMENOW -}}"
          TYPE: "job"

  scale-to-0:
    aliases: ["0"]
    desc: Scale given workloads to 0
    vars: &scale-vars
      KS:
        sh: |-
          [[ -z "{{.KS}}" ]] && echo "{{.APP}}-app" || echo "{{.KS}}"
      KSNS: '{{ .KSNS | default "flux-system" }}'
      HR: '{{ .HR | default .APP }}'
      HRNS:
        sh: |-
          [[ -n "{{.HRNS}}" ]] && echo "{{.HRNS}}" && exit || [[ -n "{{.NS}}" ]] && echo "{{.NS}}" && exit || echo "{{.APP}}"
      APP: *app-fail
      NS: '{{ .NS | default .APP }}'
      REASON: '{{ .REASON | default .TIMENOW }}'
    cmds:
      - |- # annotate Flux Kustomization
        [[ {{.KS}} != "false" ]] && kubectl annotate --overwrite kustomizations.kustomize.toolkit.fluxcd.io -n {{.KSNS}} {{.KS}} scaledown.home.arpa/reason={{.REASON}} || true
      - |- # suspend Flux Kustomization
        [[ {{.KS}} != "false" ]] && flux suspend kustomization -n {{.KSNS}} {{.KS}} || true
      - |- # annotate Flux HelmRelease
        [[ {{.HR}} != "false" ]] && kubectl annotate --overwrite helmreleases.helm.toolkit.fluxcd.io -n {{.HRNS}} {{.HR}} scaledown.home.arpa/reason={{.REASON}} || true
      - |- # suspend Flux HelmRelease
        [[ {{.HR}} != "false" ]] && flux suspend helmrelease -n {{.HRNS}} {{.HR}} || true
      - |- # annotate Kubernetes controller with reason
        kubectl annotate --overwrite deployment -n {{.NS}} {{.APP}} scaledown.home.arpa/reason={{.REASON}} || kubectl annotate --overwrite statefulset -n {{.NS}} {{.APP}} scaledown.home.arpa/reason={{.REASON}}
      - |- # annotate Kubernetes controller with old replica count
        kubectl annotate --overwrite deployment -n {{.NS}} {{.APP}} scaledown.home.arpa/replicas=$(kubectl get deployment -n {{.NS}} {{.APP}} -o jsonpath='{.spec.replicas}') || kubectl annotate --overwrite statefulset -n {{.NS}} {{.APP}} scaledown.home.arpa/replicas=$(kubectl get statefulset -n {{.NS}} {{.APP}} -o jsonpath='{.spec.replicas}')
      - |- # scale down Kubernetes controller
        kubectl scale deployment -n {{.NS}} {{.APP}} --replicas 0 || kubectl scale statefulset -n {{.NS}} {{.APP}} --replicas 0
      - task: wait-pod-delete
        vars:
          NAME: '{{.APP}}'
          NS: '{{.NS}}'

  scale-back-up:
    aliases: ["up"]
    vars: *scale-vars
    cmds:
      - |- # scale up Kubernetes controller back to old replica count
        kubectl scale deployment -n {{.NS}} {{.APP}} --replicas "$(kubectl get deployment -n {{.NS}} {{.APP}} -o jsonpath='{.metadata.annotations.scaledown\.home\.arpa/replicas}')" || kubectl scale statefulset -n {{.NS}} {{.APP}} --replicas "$(kubectl get statefulset -n {{.NS}} {{.APP}} -o jsonpath='{.metadata.annotations.scaledown\.home\.arpa/replicas}')"
      - |- # remove old replica count annotation from Kubernetes controller
        kubectl annotate deployment -n {{.NS}} {{.APP}} scaledown.home.arpa/replicas- || kubectl annotate statefulset -n {{.NS}} {{.APP}} scaledown.home.arpa/replicas-
      - |- # remove reason annotation from Kubernetes controller
        kubectl annotate deployment -n {{.NS}} {{.APP}} scaledown.home.arpa/reason- || kubectl annotate statefulset -n {{.NS}} {{.APP}} scaledown.home.arpa/reason-
      - |- # resume Flux HelmRelease
        [[ {{.HR}} != "disabled" ]] && flux resume helmrelease -n {{.HRNS}} {{.HR}} || true
      - |- # remove reason annotation from Flux HelmRelease
        [[ {{.HR}} != "disabled" ]] && kubectl annotate helmreleases.helm.toolkit.fluxcd.io -n {{.HRNS}} {{.HR}} scaledown.home.arpa/reason- || true
      - |- # resume Flux Kustomization
        [[ {{.KS}} != "disabled" ]] && flux resume kustomization -n {{.KSNS}} {{.KS}} || true
      - |- # remove reason annotation from Flux Kustomization
        [[ {{.KS}} != "disabled" ]] && kubectl annotate kustomizations.kustomize.toolkit.fluxcd.io -n {{.KSNS}} {{.KS}} scaledown.home.arpa/reason- || true
