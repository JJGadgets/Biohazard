---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
spec:
  interval: 5m

  # use rook-ceph-charts from ./kube/1-bootstrap/flux/flux-system/chart to reference package from Helm repo
  chart:
    spec:
      chart: rook-ceph-cluster
      version: v1.9.0
      sourceRef:
        kind: HelmRepository
        name: rook-ceph-charts
        namespace: flux-system

  # ensure operator is deployed before cluster
  dependsOn:
    - name: rook-ceph-operator
      namespace: rook-ceph

  # customize running values
  values:
    # CRDs taken care of by operator
    # enable monitoring on monitoring server
    monitoring:
      enabled: true

    # same as spec section in ceph.rook.io/v1 API's CephCluster CRD
    cephClusterSpec:
      # disable crash telemetry
      crashCollector:
        disable: false
      # Ceph Dashboard
      dashboard:
        enabled: true
        urlPrefix: /
        ssl: false
      # define storage topology
      storage:
        useAllNodes: false
        useAllDevices: false
        config:
          # define Ceph Object Storage Daemons, or OSDs, per device (drive/partition). equivalent of filesystem to be installed on partition
          osdsPerDevice: "1"
        # storage topology itself, define which drive/partition device to be used from which nodes
        # for Proxmox VMs, the OS virtual disk will be attached to sda via VirtIO SCSI and the Ceph virtual disk will be attached to vda via VirtIO Block
        nodes:
          - name: kube-pve-master1
            devices:
              - name: "/dev/vda"
          - name: kube-pve-master2
            devices:
              - name: "/dev/vda"
          - name: kube-pve-worker3
            devices:
              - name: "/dev/vda"

    # define Ceph block storage
    cephBlockPools:
      - name: kube-ceph-blockpool
        spec:
          # set block data replication settings
          # current config: data will be spread across 3 unique hosts/nodes rather than devices, since we have 3 nodes, if any 1 goes down the other 2 still have the storage
          failureDomain: host
          replicated:
            size: 3
        # define Kubernetes storage class to use Ceph block storage
        storageClass:
          enabled: true
          name: kube-ceph-block-storageclass
          isDefault: true
          # when pod is deleted, is storage used reclaimed (Delete) or Retained
          reclaimPolicy: Retain
          allowVolumeExpansion: true
          parameters:
            # below are all Rook Ceph defaults from https://github.com/rook/rook/blob/master/Documentation/ceph-block.md as of v1.9.0 17 April 2022
            # defaults are copied here so that in event deafults are changed, it does not automatically affect desired production state without manual review
            imageFormat: "2"
            imageFeatures: layering
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
            csi.storage.k8s.io/fstype: ext4

    # define CephFS file storage
    cephFileSystems:
      - name: kube-ceph-fs
        spec:
          # define file metadata settings
          metadataPool:
            # set file metadata replication settings
            # current config: data will be spread across 3 unique hosts/nodes rather than devices, since we have 3 nodes, if any 1 goes down the other 2 still have the storage
            failureDomain: host
            replicated:
              size: 3
          # define file data settings
          dataPools:
            - name: kube-ceph-fs-datapool1
              # set file data replication settings
              # current config: data will be spread across 3 unique hosts/nodes rather than devices, since we have 3 nodes, if any 1 goes down the other 2 still have the storage
              failureDomain: host
              replicated:
                size: 3
          # define pods used to serve CephFS metadata
          metadataServer:
            # current config: 2 pods spun up, 1 active, 1 standby failover
            activeCount: 1
            activeStandby: true
            # resource constraints
            resources:
              # TODO: bump memory up after getting more per node
              requests:
                cpu: 1000m
                memory: 1Gi
              limits:
                memory: 2Gi
        # define Kubernetes storage class to use CephFS
        storageClass:
          enabled: true
          name: kube-ceph-fs-storageclass
          isDefault: false
          # which CephFS data pool to use
          pool: kube-ceph-fs-datapool1
          # when pod is deleted, is storage used reclaimed (Delete) or Retained
          reclaimPolicy: Retain
          allowVolumeExpansion: true
          parameters:
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
            csi.storage.k8s.io/fstype: ext4

    # define Ceph object storage
    cephObjectStores:
      - name: kube-ceph-object
        spec:
          # define file metadata settings
          metadataPool:
            # set object metadata replication settings
            # current config: data will be spread across 3 unique hosts/nodes rather than devices, since we have 3 nodes, if any 1 goes down the other 2 still have the storage
            failureDomain: host
            replicated:
              size: 3
          # define object data settings
          dataPool:
            # set object data replication settings
            # current config: data will be reconstructed if any host/node is missing through erasure coding (somewhat like parity)
            failureDomain: host
            erasureCoded:
              dataChunks: 2
              codingChunks: 1
          # retention policy for deleting data pools
          preservePoolsOnDelete: true
          gateway:
            port: 80
            resources:
              requests:
                cpu: 1000m
                memory: 1Gi
              limits:
                memory: 2Gi
            instances: 1
          healthCheck:
            bucket:
              interval: 60s
        storageClass:
          enabled: true
          name: kube-ceph-s3
          reclaimPolicy: Retain
          # rep Singapore, big up
          parameters:
            region: ap-southeast-1

# boring stuff
  # customize install process
  install:
    # CRDs taken care of by operator
    # create namespace in case the namespace manifests aren't applied
    createNamespace: true
    remediation:
      retries: 5

  # customize upgrade process
  upgrade:
    # CRDs taken care of by operator
    remediation:
      retries: 5
