---
# yaml-language-server: $schema=https://crds.jank.ing/postgres-operator.crunchydata.com/postgrescluster_v1beta1.json
apiVersion: postgres-operator.crunchydata.com/v1beta1
kind: PostgresCluster
metadata:
  name: &name "pg-${PG_APP_NAME}"
  namespace: "${PG_APP_NS}"
spec:
  postgresVersion: 16
  patroni: # turn on sync writes to at least 1 other replica
    dynamicConfiguration:
      synchronous_mode: true
      member_slots_ttl: "1440min"
      postgresql:
        synchronous_commit: "on"
        use_slots: "true"
        huge_page_size: 1GB # the hugepages type (2Mi vs 1Gi), not amount allocated
  instances:
    - name: &instance "${PG_APP_NAME}"
      metadata:
        labels:
          egress.home.arpa/apiserver: "allow"
          #egress.home.arpa/r2: "allow" # TODO: this uses L7 DNS netpols, and each WAL push makes a new DNS request, on top of recursing ndots per request, causing Cilium dnsproxy to crash
          egress.home.arpa/world: "allow"
          s3.home.arpa/store: "rgw-${CLUSTER_NAME}"
          s3.home.arpa/minio-nas: "allow"
          prom.home.arpa/kps: "allow"
      replicas: ${PG_REPLICAS:=3}
      dataVolumeClaimSpec:
        storageClassName: "${PG_SC:=local}"
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: "${PG_CONFIG_SIZE:=20Gi}"
      resources:
        requests:
          cpu: "${PG_CPU_REQ:=50m}"
          memory: "${PG_MEM_REQ:=512M}"
        limits:
          cpu: "${PG_CPU_LIMIT:=2000m}"
          memory: "${PG_MEM_LIMIT:=2Gi}"
          "hugepages-${PG_HUGEPAGES_SIZE:=1Gi}": "${PG_HUGEPAGES_REQ:=1Gi}"
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: node-role.kubernetes.io/control-plane
                    operator: Exists
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: "kubernetes.io/hostname"
          whenUnsatisfiable: "DoNotSchedule"
          labelSelector:
            matchLabels:
              postgres-operator.crunchydata.com/cluster: *name
              postgres-operator.crunchydata.com/instance-set: *instance
  users:
    - name: "${PG_DB_USER:=app}"
      databases: ["${PG_DB_NAME:=app}"]
  backups:
    pgbackrest:
      metadata:
        labels:
          egress.home.arpa/apiserver: "allow"
          #egress.home.arpa/r2: "allow" # TODO: this uses L7 DNS netpols, and each WAL push makes a new DNS request, on top of recursing ndots per request, causing Cilium dnsproxy to crash
          egress.home.arpa/world: "allow"
          s3.home.arpa/store: "rgw-${CLUSTER_NAME}"
          s3.home.arpa/minio-nas: "allow"
      configuration: &brcfg
        - secret:
            name: "pg-${PG_APP_NAME}-secrets"
        - secret:
            name: "pg-${PG_APP_NAME}-s3-crunchy"
      #manual:
      #  repoName: "repo2"
      #  options: ["--type=full", "--annotation=reason=change-R2-buckets", "--archive-copy", "--checksum-page"]
      global: &brflag
        archive-timeout: "60"
        compress-type: "bz2"
        compress-level: "9"
        #repo1-bundle: "y"
        #repo1-block: "y"
        #repo1-retention-full-type: "time"
        #repo1-retention-full: "15" # keep 2 weeks of backups
        ##repo1-retention-diff-type: "time"
        #repo1-retention-diff: "7"
        ##repo1-path: "/pgbackrest/repo1/${PG_APP_NAME}" # NFS
        #repo1-path: "/${PG_APP_NAME}"
        #repo1-s3-uri-style: "path"
        # repo2-storage-host: "${SECRET_PGBACKREST_WAL_R2_ENDPOINT}."
        repo2-bundle: "y"
        repo2-block: "y"
        repo2-path: "/${PG_APP_NAME}"
        repo2-s3-uri-style: "path"
        repo2-retention-full-type: "time"
        repo2-retention-full: "15" # keep 2 weeks of backups
        repo2-cipher-type: "aes-256-cbc"
        # repo3-storage-host: "${APP_DNS_RGW_HTTPS}."
        repo3-bundle: "y"
        repo3-block: "y"
        repo3-s3-uri-style: "path"
        repo3-retention-full-type: "time"
        repo3-retention-full: "5"
        repo3-retention-diff: "30"
        repo3-cipher-type: "aes-256-cbc"
        log-level-stderr: "debug"
        log-level-console: "debug"
      repos:
        - name: "repo3" # Ceph RGW in-cluster
          s3: &rgw
            endpoint: "${APP_DNS_RGW_HTTPS}" # trailing dot to prevent ndots
            bucket: "pg-${PG_APP_NAME}"
            region: "us-east-1"
          schedules: # times staggered to avoid NFS schedule causing failed jobs due to locks
            full: "15 6 * * 1" # every Monday at 06:15
            differential: "15 6 * * 0,2-6" # every day at 06:15 except Monday
            incremental: "15 1-5,7-23 * * *" # every hour except 06:15
        #- name: "repo1" # NFS
        #  s3: &minio
        #    endpoint: "${APP_DNS_MINIO_NAS_S3}"
        #    bucket: "${SECRET_PGBACKREST_WAL_MINIO_BUCKET}"
        #    region: "us-east-1"
        #  #volume: &nfs
        #  #  volumeClaimSpec:
        #  #    storageClassName: "pg-${PG_APP_NAME}-wal-nfs"
        #  #    volumeName: "pg-${PG_APP_NAME}-wal-nfs"
        #  #    accessModes: ["ReadWriteMany"]
        #  #    resources:
        #  #      requests:
        #  #        storage: "1Mi" # doesn't matter with NFS
        #  schedules: # more aggressive for NAS (over NFS)
        #    full: "0 6 * * 1" # every Monday at 06:00
        #    differential: "0 6 * * 2-6" # every day at 06:00 except Monday
        - name: "repo2" # Cloudflare R2
          s3: &r2
            endpoint: "${SECRET_PGBACKREST_WAL_R2_ENDPOINT}"
            bucket: "${SECRET_PGBACKREST_WAL_R2_BUCKET}-${PG_APP_NAME}"
            region: "us-east-1"
          schedules: # times staggered to avoid NFS schedule causing failed jobs due to locks
            full: "30 6 * * 1" # every Monday at 06:30
            incremental: "30 6 * * 0,2-6" # every day at 06:30 except Monday
      jobs:
        ttlSecondsAfterFinished: 60
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              - topologyKey: kubernetes.io/hostname
                namespaceSelector: {}
                labelSelector:
                  matchExpressions:
                    - key: postgres-operator.crunchydata.com/pgbackrest-cronjob
                      operator: Exists
        resources:
          requests:
            cpu: 10m
          limits:
            cpu: 1000m
            memory: 100Mi
  dataSource:
    pgbackrest:
      stanza: "db"
      configuration: *brcfg
      global: *brflag
      repo:
        name: "repo3"
        s3: *rgw
  proxy:
    pgBouncer:
      port: 5432
      replicas: 3
      # config:
      #   global:
      #     pool_mode: "transaction"
      resources:
        requests:
          cpu: 10m
        limits:
          cpu: 1000m
          memory: 100Mi
      sidecars:
        pgbouncerConfig:
          resources:
            requests:
              cpu: 5m
            limits:
              cpu: 1000m
              memory: 10Mi
      tolerations:
        - key: "node-role.kubernetes.io/control-plane"
          operator: "Exists"
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: "kubernetes.io/hostname"
          whenUnsatisfiable: "DoNotSchedule"
          labelSelector:
            matchLabels:
              postgres-operator.crunchydata.com/cluster: *name
              postgres-operator.crunchydata.com/role: "pgbouncer"
  monitoring:
    pgmonitor:
      exporter:
        resources:
          requests:
            cpu: 10m
            memory: 64M
          limits:
            cpu: 1000m
            memory: 512M
---
apiVersion: v1
kind: Secret
metadata:
  name: "pg-${PG_APP_NAME}-secrets"
  namespace: "${PG_APP_NS}"
type: Opaque
stringData:
    #repo1-s3-key=${SECRET_PGBACKREST_WAL_MINIO_ID}
    #repo1-s3-key-secret=${SECRET_PGBACKREST_WAL_MINIO_KEY}
  s3.conf: |
    [global]
    repo2-s3-key=${SECRET_PGBACKREST_WAL_R2_ID}
    repo2-s3-key-secret=${SECRET_PGBACKREST_WAL_R2_KEY}
  encryption.conf: |
    [global]
    repo2-cipher-pass=${SECRET_PGBACKREST_WAL_ENCRYPT}
    repo3-cipher-pass=${SECRET_PGBACKREST_WAL_ENCRYPT}
