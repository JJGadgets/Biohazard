---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
spec:
  timeout: 15m
  chart:
    spec:
      chart: rook-ceph-cluster
      version: "${VERSION_ROOK:=v1.10.10}"
      sourceRef:
        name: rook-ceph
        kind: HelmRepository
        namespace: flux-system
  values:
    clusterName: "${CLUSTER_NAME}"
    operatorNamespace: "rook-ceph"
    configOverride: |
      [global]
      bdev_enable_discard = true
      bdev_async_discard = true
      osd_class_update_on_start = false
      mon_data_avail_warn = 10
    cephClusterSpec:
      # cephVersion:
      #   image: "quay.io/ceph/ceph:v17.2.3" 
      network:
        # provider: host
        connections:
          encryption:
            enabled: true
          compression:
            enabled: true
      crashCollector:
        disable: true
      dashboard:
        enabled: true
        urlPrefix: "/"
        ssl: false
      mon:
        # count: 3
        count: 1
        allowMultiplePerNode: false
      mgr:
        # count: 2
        count: 1
        allowMultiplePerNode: false
        modules:
          - name: "pg_autoscaler"
            enabled: true
      removeOSDsIfOutAndSafeToRemove: false
      storage:
        useAllNodes: false
        useAllDevices: false
        config:
          # encryptedDevice: "true" # TODO: temporarily commented, OSD prepare with encrypted OSDs fails on >v17.2.3: https://github.com/rook/rook/issues/11304#issuecomment-1321286046
          osdsPerDevice: "1"
        nodes:
          - name: "humming"
            devicePathFilter: "^/dev/disk/by-id/ata-INTEL_SSDSC2BB016T4.*"
          - name: "blackfish"
            devices:
              - name: "/dev/disk/by-id/ata-INTEL_SSDSC2BB016T4_BTWD709202JK1P6HGN"
                config:
                  encryptedDevice: "true"
            # devicePathFilter: "^/dev/disk/by-id/ata-INTEL_SSDSC2BB016T4.*"
          # - name: "strato"
          #   devicePathFilter: "^/dev/disk/by-id/ata-INTEL_SSDSC2BB016T4.*"
    cephBlockPools:
      - name: &rbd "${CLUSTER_NAME}-block-k8s-ssd"
        spec:
          failureDomain: "osd"
          deviceClass: "ssd"
          replicated:
            # size: 3
            size: 2
          parameters:
            min_size: "2"
            compression_mode: "aggressive"
            compression_algorithm: "zstd"
        storageClass:
          enabled: true
          name: "block"
          isDefault: true
          reclaimPolicy: "Delete"
          allowVolumeExpansion: true
          mountOptions: ["discard"]
          parameters:
            imageFormat: "2"
            imageFeatures: "layering,exclusive-lock,object-map,fast-diff,deep-flatten" # https://docs.ceph.com/en/quincy/rbd/rbd-config-ref/#image-features
            csi.storage.k8s.io/provisioner-secret-name: "rook-csi-rbd-provisioner"
            csi.storage.k8s.io/provisioner-secret-namespace: "rook-ceph"
            csi.storage.k8s.io/controller-expand-secret-name: "rook-csi-rbd-provisioner"
            csi.storage.k8s.io/controller-expand-secret-namespace: "rook-ceph"
            csi.storage.k8s.io/node-stage-secret-name: "rook-csi-rbd-node"
            csi.storage.k8s.io/node-stage-secret-namespace: "rook-ceph"
            csi.storage.k8s.io/fstype: "ext4"
    cephBlockPoolsVolumeSnapshotClass:
      enabled: true
      name: *rbd
      isDefault: true
      deletionPolicy: Delete
    cephFileSystems: []
      # - name: &fs "${CLUSTER_NAME}-fs"
      #   spec:
      #     preserveFilesystemOnDelete: true
      #     metadataPool:
      #       replicated:
      #         # size: 3
      #         size: 2
      #     dataPools:
      #       - name: &fsdata0 "${CLUSTER_NAME}-fs-data0"
      #         failureDomain: "osd"
      #         replicated:
      #           # size: 3
      #           size: 2
      #         parameters:
      #           compression_mode: "aggressive"
      #           compression_algorithm: "zstd"
      #     metadataServer:
      #       activeCount: 1
      #       activeStandby: true
      #       resources:
      #         requests:
      #           cpu: 1000m
      #           memory: 4Gi
      #         limits:
      #           memory: 4Gi
      #   storageClass:
      #     enabled: true
      #     isDefault: false
      #     name: "file"
      #     pool: *fsdata0
      #     reclaimPolicy: "Delete"
      #     allowVolumeExpansion: true
      #     mountOptions: ["discard"]
      #     parameters:
      #       csi.storage.k8s.io/provisioner-secret-name: "rook-csi-cephfs-provisioner"
      #       csi.storage.k8s.io/provisioner-secret-namespace: "rook-ceph"
      #       csi.storage.k8s.io/controller-expand-secret-name: "rook-csi-cephfs-provisioner"
      #       csi.storage.k8s.io/controller-expand-secret-namespace: "rook-ceph"
      #       csi.storage.k8s.io/node-stage-secret-name: "rook-csi-cephfs-node"
      #       csi.storage.k8s.io/node-stage-secret-namespace: "rook-ceph"
      #       csi.storage.k8s.io/fstype: "ext4"
    cephFileSystemVolumeSnapshotClass:
      enabled: false
      # name: *fs
      isDefault: false
      deletionPolicy: Delete
    cephObjectStores: []
      # - name: rgw-${CLUSTER_NAME}
      #   spec:
      #     preservePoolsOnDelete: true
      #     metadataPool:
      #       failureDomain: host
      #       replicated:
      #         size: 3
      #     dataPool:
      #       failureDomain: host
      #       replicated:
      #         size: 3
      #     gateway:
      #       port: 80
      #       resources:
      #         requests:
      #           cpu: 250m
      #           memory: 1Gi
      #         limits:
      #           memory: 2Gi
      #       instances: 2
      #     healthCheck:
      #       bucket:
      #         interval: 60s
      #   storageClass:
      #     enabled: true
      #     name: ceph-bucket
      #     reclaimPolicy: Delete
      #     parameters:
      #       region: us-east-1
    monitoring:
      enabled: true
      createPrometheusRules: true
    ingress:
      dashboard:
        ingressClassName: nginx
        host:
          name: &host "${APP_DNS_CEPH}"
          path: /
        tls:
          - hosts:
              - *host
    pspEnable: false