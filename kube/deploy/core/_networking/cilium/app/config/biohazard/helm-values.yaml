---
# yaml-language-server: $schema=https://raw.githubusercontent.com/cilium/cilium/refs/tags/v1.16.4/install/kubernetes/cilium/values.schema.json

## NOTE: required for Talos
securityContext:
  capabilities:
    ciliumAgent: [CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,NET_BIND_SERVICE,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID]
    cleanCiliumState: [NET_ADMIN,SYS_ADMIN,SYS_RESOURCE]
cgroup:
  autoMount:
    enabled: false
  hostRoot: "/sys/fs/cgroup"

## NOTE: Cluster identification, mainly for ClusterMesh
cluster:
  name: "biohazard"
  id: 1

## NOTE: Cilium's routing modes for inter-nodes pod traffic
routingMode: native
devices: 'br0'
autoDirectNodeRoutes: true
ipv4NativeRoutingCIDR: "${IP_POD_CIDR_V4}"
endpointRoutes: # supposedly helps with LB routing...? 1.16 introduced a bug where BGP LBs (L2 untested) would randomly timeout requests at unknown intervals, most noticeably is loading SearXNG front page would usually load practically instantly but would be stuck until timeout, FortiGate pcaps show connection does establish but TCP Previous Segment Not Captured
  enabled: true
loadBalancer:
  algorithm: maglev
  mode: dsr

## NOTE: Cilium's networking internals
ipam:
  mode: kubernetes
kubeProxyReplacement: true
### Talos 1.5 and above come with KubePrism which is an internal TCP load balancer for kube-apiserver. DO NOT COPY IF NOT ON TALOS OR A KUBEPRISM-SUPPORTED KUBERNETES DISTRIBUTION!!!
k8sServiceHost: "127.0.0.1"
k8sServicePort: "7445"
kubeProxyReplacementHealthzBindAddr: "0.0.0.0:10256"

## Multus compatibility
cni:
  exclusive: false

## NOTE: Cilium can automatically kill and respawn pods upon ConfigMap updates or other resource changes
rollOutCiliumPods: true
operator:
  rollOutPods: true

## NOTE: Cilium L2 LoadBalancer service IP announcements # disabled since it seems to cause noticeable apiserver usage increase to the point of causing stuck endpoint creation
externalIPs:
  enabled: false
l2announcements:
  enabled: false

## NOTE: Cilium additional features and/or CRDs
bpf:
  masquerade: true
  # hostLegacyRouting: true # so pods can use the normal Linux routing table from the host
  tproxy: true # L7 netpols stuff
  preallocateMaps: true # reduce latency, increased memory usage
  policyMapMax: 40960 # 2.5x default, Increase Cilium map sizes due to amount of netpols and identities, when BPF map pressure hits 100 endpoint creation starts failing, max dynamic size ratio doesn't increase this
  enableTCX: true # testing if it causes Cilium 1.16 BGP LB timeouts
l7Proxy: true # enables L7 netpols (including DNS) via proxy, e.g. Envoy
socketLB:
  enabled: true # faster and more direct same-node pod routing than tc/tcx # supposed to be default off, but it's enabled anyway if unspecified, and looks fun lol
  #hostNamespaceOnly: true # KubeVirt compatibility with k8s services # disabled because KubeVirt VMs now use Multus bridging rather than CNI

bgpControlPlane:
  enabled: true
  ### `bgpControlPlane.enabled: true` is newer GoBGP implementation, while `bgp.enabled: true` and `bgp.announce` uses older MetalLB BGP implementation that is planned to be deprecated in Cilium v1.15.
  ### `bgp.announce` block is replaced by CiliumBGPPeeringPolicy CRD used by bgpControlPlane, for more fine grained control over announced addresses
localRedirectPolicy: false
nodePort:
  enabled: false
bandwidthManager:
  enabled: false
  bbr: false # enable after Talos kernel updated to >= 5.18
enableIPv4BIGTCP: true
enableIPv6BIGTCP: true

## NOTE: Hubble observability
hubble:
  enabled: true
  peerService:
    clusterDomain: cluster.local
  relay:
    enabled: true
    rollOutPods: true
  ui:
    enabled: true
    rollOutPods: true

### endpointStatus + enableCnpStatusUpdates no longer enabled since it can cause large apiserver resource usage and latency spikes, removed from Cilium 1.16, since netpols now have validation status
