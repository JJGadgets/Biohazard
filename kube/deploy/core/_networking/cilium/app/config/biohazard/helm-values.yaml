---
# yaml-language-server: $schema=https://raw.githubusercontent.com/cilium/cilium/refs/tags/v1.17.5/install/kubernetes/cilium/values.schema.json

## NOTE: required for Talos
securityContext:
  capabilities:
    ciliumAgent: [CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,NET_BIND_SERVICE,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID]
    cleanCiliumState: [NET_ADMIN,SYS_ADMIN,SYS_RESOURCE]
cgroup:
  autoMount:
    enabled: false
  hostRoot: "/sys/fs/cgroup"

## NOTE: Cluster identification, mainly for ClusterMesh
cluster:
  name: "biohazard"
  id: 1

## NOTE: Cilium's routing modes for inter-nodes pod traffic
routingMode: native
devices: 'br0'
autoDirectNodeRoutes: true
ipv4NativeRoutingCIDR: "${IP_POD_CIDR_V4}"
endpointRoutes: # supposedly helps with LB routing...?
  enabled: true
loadBalancer:
  algorithm: maglev
  mode: dsr

## NOTE: Cilium's networking internals
ipam:
  mode: kubernetes
kubeProxyReplacement: true
### Talos 1.5 and above come with KubePrism which is an internal TCP load balancer for kube-apiserver. DO NOT COPY IF NOT ON TALOS OR A KUBEPRISM-SUPPORTED KUBERNETES DISTRIBUTION!!!
k8sServiceHost: "127.0.0.1"
k8sServicePort: "7445"
kubeProxyReplacementHealthzBindAddr: "0.0.0.0:10256"
enableIPv4Masquerade: false # BGP advertise PodCIDR so only FortiGate does NAT
directRoutingSkipUnreachable: true # use local L2 within cluster while outside cluster uses BGP
bpf:
  # masquerade: true
  # hostLegacyRouting: true # so pods can use the normal Linux routing table from the host
  tproxy: true # L7 netpols stuff
  preallocateMaps: true # reduce latency, increased memory usage
  policyMapMax: 40960 # 2.5x default, Increase Cilium map sizes due to amount of netpols and identities, when BPF map pressure hits 100 endpoint creation starts failing, max dynamic size ratio doesn't increase this
l7Proxy: true # enables L7 netpols (including DNS) via proxy, e.g. Envoy
socketLB:
  enabled: true # faster and more direct same-node pod routing than tc/tcx # supposed to be default off, but it's enabled anyway if unspecified, and looks fun lol
  hostNamespaceOnly: true # KubeVirt, gvisor and Kata compatibility with k8s services
enableIPv4BIGTCP: true
enableIPv6BIGTCP: true
bandwidthManager:
  enabled: true
  bbr: true
localRedirectPolicy: false
nodePort:
  enabled: false

## Multus compatibility
cni:
  exclusive: false

## NOTE: Cilium can automatically kill and respawn pods upon ConfigMap updates or other resource changes
rollOutCiliumPods: true
operator:
  rollOutPods: true

## NOTE: Cilium L2 LoadBalancer service IP announcements # disabled since it seems to cause noticeable apiserver usage increase to the point of causing stuck endpoint creation
externalIPs:
  enabled: true
l2announcements:
  enabled: true

## NOTE: Cilium additional features and/or CRDs

bgpControlPlane:
  enabled: true
  ### `bgpControlPlane.enabled: true` is newer GoBGP implementation, while `bgp.enabled: true` and `bgp.announce` uses older MetalLB BGP implementation that is planned to be deprecated in Cilium v1.15.
  ### `bgp.announce` block is replaced by CiliumBGPPeeringPolicy CRD used by bgpControlPlane, for more fine grained control over announced addresses

## NOTE: Hubble observability
hubble:
  enabled: true
  peerService:
    clusterDomain: cluster.local
  relay:
    enabled: true
    rollOutPods: true
  ui:
    enabled: true
    rollOutPods: true

## NOTE: ingress/gateway
ingressController:
  enabled: false
  # enforceHttps: true
  # loadbalancerMode: shared
  # defaultSecretNamespace: ingress
  # defaultSecretName: short-domain-tls
  # service:
  #   annotations:
  #     lbipam.cilium.io/ips: "${APP_IP_CILIUM_INGRESS:=127.0.0.1}"

gatewayAPI:
  enabled: false
  # enableAlpn: true
  # enableAppProtocol: true
  # xffNumTrustedHops: 1
