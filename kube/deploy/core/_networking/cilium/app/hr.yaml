---
# yaml-language-server: $schema=https://flux.jank.ing/helmrelease/v2/github/cilium/cilium/v1.16.5/install/kubernetes/cilium
apiVersion: helm.toolkit.fluxcd.io/v2beta2
kind: HelmRelease
metadata:
  name: cilium
  namespace: kube-system
  annotations:
    meta.helm.sh/release-name: cilium
    meta.helm.sh/release-namespace: kube-system
  labels:
    app.kubernetes.io/managed-by: Helm
spec:
  interval: "5m"
  timeout: "1h"
  chart:
    spec:
      chart: cilium
      version: "1.17.3"
      sourceRef:
        name: cilium-charts
        kind: HelmRepository
        namespace: flux-system
  valuesFrom:
    - kind: "ConfigMap"
      name: "cilium-helm-values"
      valuesKey: "${CLUSTER_NAME:=biohazard}.yaml"
      optional: false
  values:
    ## NOTE: BGP for LoadBalancer services
    ### `bgpControlPlane.enabled: true` is newer GoBGP implementation, while `bgp.enabled: true` and `bgp.announce` uses older MetalLB BGP implementation that is planned to be deprecated in Cilium v1.15.
    ### `bgp.announce` block is replaced by CiliumBGPPeeringPolicy CRD used by bgpControlPlane, for more fine grained control over announced addresses
    # bgp:
    #   enabled: true
    #   announce:
    #     loadbalancerIP: true
    #     podCIDR: true
    bgpControlPlane:
      enabled: true
    ## Note: dedicated Envoy, easier resource requests
    envoy:
      enabled: true
    ## Note: Cilium ingress controller
    ingressController:
      enabled: false
      #enforceHttps: true
      #loadbalancerMode: "shared"
      #defaultSecretName: "short-domain-tls"
      #defaultSecretNamespace: "ingress"
      #service:
      #  type: "LoadBalancer"
      #  annotations:
      #    "io.cilium/lb-ipam-ips": "${APP_IP_CILIUM_INGRESS}"
    ## NOTE: Observability with Hubble & Prometheus
    hubble:
      ui:
        ingress:
          enabled: true
          className: "nginx-internal"
          hosts:
            - "${APP_DNS_HUBBLE:=hubble}"
          tls:
            - hosts:
                - "${APP_DNS_HUBBLE:=hubble}"
      relay:
        prometheus:
          enabled: true
          serviceMonitor:
            enabled: true
      metrics:
        enabled:
          - dns:query;labelsContext=source_pod,source_workload,source_namespace,destination_ip,destination_pod,destination_workload,destination_namespace,traffic_direction
          - drop:labelsContext=source_pod,source_workload,source_namespace,destination_ip,destination_pod,destination_workload,destination_namespace,traffic_direction
          - tcp:labelsContext=source_pod,source_workload,source_namespace,destination_ip,destination_pod,destination_workload,destination_namespace,traffic_direction
          - flow:labelsContext=source_pod,source_workload,source_namespace,destination_ip,destination_pod,destination_workload,destination_namespace,traffic_direction
          - flows-to-world:any-drop;port;labelsContext=source_pod,source_workload,source_namespace,destination_ip,destination_pod,destination_workload,destination_namespace,traffic_direction
          - port-distribution:labelsContext=source_pod,source_workload,source_namespace,destination_ip,destination_pod,destination_workload,destination_namespace,traffic_direction
          - httpV2:labelsContext=source_pod,source_workload,source_namespace,destination_ip,destination_pod,destination_workload,destination_namespace,traffic_direction
        serviceMonitor:
          enabled: true
          interval: "60s"
          relabelings:
            # replaces other node identifiers with hostname
            - action: replace
              regex: (.*)
              replacement: $1
              sourceLabels:
                - __meta_kubernetes_pod_node_name
              targetLabel: kubernetes_node
            - action: replace
              regex: (.*)
              replacement: $1
              sourceLabels:
                - __meta_kubernetes_pod_node_name
              targetLabel: nodename
            - action: replace
              regex: (.*)
              replacement: $1.${CLUSTER_NAME}:9100
              sourceLabels:
                - kubernetes_node
              targetLabel: instance
        dashboards:
          enabled: false # managed in Grafana HR to upgrade queries
          #enabled: true
          #annotations:
          #  grafana_folder: "Cilium"
      eventQueueSize: "50000" # default is 6144 which fills up
    operator:
      prometheus:
        enabled: true
        serviceMonitor:
          enabled: true
      dashboards:
        enabled: true
        annotations:
          grafana_folder: "Cilium"
    prometheus:
      enabled: true
      serviceMonitor:
        enabled: true
        trustCRDsExist: true # for CI or local Helm builds, such as flux-local
    dashboards:
      enabled: true
      annotations:
        grafana_folder: "Cilium"
    resources: # for agent
      requests:
        cpu: "100m"
        memory: "512Mi"
      limits:
        cpu: "2"
        memory: "6Gi"
