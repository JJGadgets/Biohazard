---
# yaml-language-server: $schema=https://raw.githubusercontent.com/bjw-s/helm-charts/common-3.4.0/charts/other/app-template/schemas/helmrelease-helm-v2beta2.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2beta2
kind: HelmRelease
metadata:
  name: &app mlc-llm
  namespace: *app
spec:
  interval: 5m
  chart:
    spec:
      chart: app-template
      version: 3.4.0
      sourceRef:
        name: bjw-s
        kind: HelmRepository
        namespace: flux-system
  values:
    controllers:
      llama3: &deploy
        type: deployment
        replicas: 1
        strategy: RollingUpdate
        pod:
          labels:
            ingress.home.arpa/nginx-internal: allow
        containers:
          main: &mlc
            image: &img
              repository: jank.ing/jjgadgets/mlc-llm
              tag: rolling@sha256:ac45fb1a375f79108f4df830a6cd44ae5e086bc99f293b70f6ccfe052023fb49
            args: ["HF://mlc-ai/$(MODEL)"]
            env: &envMain
              TZ: "${CONFIG_TZ}"
              MLC_JIT_POLICY: "READONLY"
              MLC_DOWNLOAD_CACHE_POLICY: "READONLY"
              MODEL: "Llama-3-8B-Instruct-q4f16_1-MLC"
            securityContext: &sc
              readOnlyRootFilesystem: true
              allowPrivilegeEscalation: false
              capabilities:
                drop: ["ALL"]
            resources: &resources
              requests:
                cpu: "10m"
              limits:
                cpu: "1000m"
                memory: "12Gi"
                gpu.intel.com/i915: "1"
            probes:
              liveness:
                enabled: true
              readiness:
                enabled: true
              startup:
                enabled: true
                custom: true
                spec:
                  periodSeconds: 2
                  failureThreshold: 300
                  tcpSocket:
                    port: 8080
      llama3-pull: &job
        type: cronjob
        cronjob:
          schedule: "@daily"
          concurrencyPolicy: "Replace"
        pod:
          labels:
            egress.home.arpa/internet: allow
        containers:
          main: &pull
            image: *img
            command: ["tini", "-g", "--", "/bin/bash", "-c"]
            args:
              - |
                if [ -d "/app/.cache/mlc_llm/model_weights/hf/mlc_ai/$(MODEL)" ] && [ -z "$(ls -A "/app/.cache/mlc_llm/model_weights/hf/mlc_ai/$(MODEL)")" ]; then
                  true;
                else
                  echo '/exit' | mlc_llm chat HF://mlc-ai/$(MODEL)
                fi
            env: &envPull
              TZ: "${CONFIG_TZ}"
              MLC_JIT_POLICY: "ON"
              MLC_DOWNLOAD_CACHE_POLICY: "ON"
              MODEL: "Llama-3-8B-Instruct-q4f16_1-MLC"
            securityContext: *sc
            resources: *resources
      codellama:
        <<: *deploy
        containers:
          main:
            <<: *mlc
            env:
              <<: *envMain
              MODEL: "CodeLlama-7b-hf-q4f32_1-MLC"
      codellama-pull:
        <<: *job
        containers:
          main:
            <<: *pull
            env:
              <<: *envPull
              MODEL: "CodeLlama-7b-hf-q4f32_1-MLC"
    service:
      llama3: &svc
        controller: llama3
        ports:
          http:
            port: 8080
            protocol: HTTP
            appProtocol: http
      codellama:
        <<: *svc
        controller: codellama
    ingress:
      llama3:
        className: nginx-internal
        hosts:
          - host: &host "llama3.${DNS_SHORT}"
            paths: &paths
              - path: /
                pathType: Prefix
                service:
                  identifier: llama3
                  port: http
        tls:
          - hosts: [*host]
      codellama:
        className: nginx-internal
        hosts:
          - host: &host "codellama.${DNS_SHORT}"
            paths: &paths
              - path: /
                pathType: Prefix
                service:
                  identifier: codellama
                  port: http
        tls:
          - hosts: [*host]
    persistence:
      misc:
        existingClaim: mlc-llm-misc
        globalMounts:
          - subPath: cache
            path: /app/.cache
          - subPath: testdata
            path: /app/.tvm_test_data
          # - subPath: tmp
          #   path: /tmp # used for downloading models, so why not download straight to disk
      tmp:
        type: emptyDir
        globalMounts:
          - subPath: tmp
            path: /tmp
    defaultPodOptions:
      automountServiceAccountToken: false
      enableServiceLinks: false
      hostAliases:
        - ip: "${APP_IP_AUTHENTIK:=127.0.0.1}"
          hostnames: ["${APP_DNS_AUTHENTIK:=authentik}"]
      securityContext:
        runAsNonRoot: true
        runAsUser: &uid ${APP_UID_MLC_LLM:=1000}
        runAsGroup: *uid
        fsGroup: *uid
        fsGroupChangePolicy: Always
        supplementalGroups: [44, 226] # iGPU
        seccompProfile: { type: "Unconfined" } # GPU hangs with RuntimeDefault
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: *app
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: fuckoff.home.arpa/mlc-llm
                    operator: DoesNotExist
