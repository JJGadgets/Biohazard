---
# yaml-language-server: $schema=https://raw.githubusercontent.com/bjw-s/helm-charts/app-template-3.7.1/charts/other/app-template/schemas/helmrelease-helm-v2beta2.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2beta2
kind: HelmRelease
metadata:
  name: &app mlc-llm
  namespace: *app
spec:
  interval: 5m
  chart:
    spec:
      chart: app-template
      version: 3.7.1
      sourceRef:
        name: bjw-s
        kind: HelmRepository
        namespace: flux-system
  values:
    controllers:
      llama3: &deploy
        type: deployment
        replicas: 1
        strategy: RollingUpdate
        pod:
          labels:
            ingress.home.arpa/nginx-internal: allow
        containers:
          main: &mlc
            image: &img
              repository: jank.ing/jjgadgets/mlc-llm
              tag: 0.19.0@sha256:acbe4da65245cdc424eb16de4dd09b6c77fc1dc48f871f04faca4c0365341420
            args: ["HF://mlc-ai/$(MODEL)"]
            env: &envMain
              TZ: "${CONFIG_TZ}"
              MLC_JIT_POLICY: "ON"
              MLC_DOWNLOAD_CACHE_POLICY: "READONLY"
              MODEL: &llama3-model "Llama-3.1-8B-Instruct-q4f16_1-MLC"
            securityContext: &sc
              readOnlyRootFilesystem: true
              allowPrivilegeEscalation: false
              capabilities:
                drop: ["ALL"]
            resources: &resources
              requests:
                cpu: "10m"
              limits:
                cpu: "1000m"
                memory: "11Gi"
                gpu.intel.com/i915: "1"
            probes:
              liveness:
                enabled: true
              readiness:
                enabled: true
              startup:
                enabled: true
                custom: true
                spec:
                  periodSeconds: 2
                  failureThreshold: 300
                  tcpSocket:
                    port: 8080
      llama3-pull: &job
        type: cronjob
        cronjob:
          schedule: "@weekly"
          concurrencyPolicy: "Replace"
        pod:
          labels:
            egress.home.arpa/internet: allow
        containers:
          main: &pull
            image: *img
            command: ["tini", "-g", "--", "/bin/bash", "-c"]
            args: ["rm -rf /tmp/* && echo '/exit' | mlc_llm chat HF://mlc-ai/$(MODEL) || true; rm -rf /tmp/*"]
            env: &envPull
              TZ: "${CONFIG_TZ}"
              MLC_JIT_POLICY: "OFF" # do on runtime
              MLC_DOWNLOAD_CACHE_POLICY: "ON"
              MODEL: *llama3-model
            securityContext: *sc
            resources:
              requests:
                cpu: "10m"
              limits:
                cpu: "1000m"
                memory: "2Gi"
      # codellama:
      #   <<: *deploy
      #   containers:
      #     main:
      #       <<: *mlc
      #       env:
      #         <<: *envMain
      #         MODEL: &codellama-model "CodeLlama-7b-hf-q4f32_1-MLC"
      #       resources:
      #         requests:
      #           cpu: "10m"
      #         limits:
      #           cpu: "1000m"
      #           memory: "12Gi"
      #           gpu.intel.com/i915: "1"
      # codellama-pull:
      #   <<: *job
      #   containers:
      #     main:
      #       <<: *pull
      #       env:
      #         <<: *envPull
      #         MODEL: *codellama-model
      phi3:
        <<: *deploy
        containers:
          main:
            <<: *mlc
            env:
              <<: *envMain
              MODEL: &phi3-model "Phi-3.5-mini-instruct-q4f16_1-MLC"
            resources:
              requests:
                cpu: "10m"
              limits:
                cpu: "1000m"
                memory: "6Gi"
                gpu.intel.com/i915: "1"
      phi3-pull:
        <<: *job
        containers:
          main:
            <<: *pull
            env:
              <<: *envPull
              MODEL: *phi3-model
      r1-qwen-32b:
        <<: *deploy
        containers:
          main:
            <<: *mlc
            env:
              <<: *envMain
              MODEL: &r1-qwen-32b-model "DeepSeek-R1-Distill-Qwen-32B-q4f16_1-MLC"
            resources:
              requests:
                cpu: "10m"
              limits:
                cpu: "1000m"
                memory: "40Gi"
                gpu.intel.com/i915: "1"
      phi3-pull:
        <<: *job
        containers:
          main:
            <<: *pull
            env:
              <<: *envPull
              MODEL: *phi3-model
    service:
      llama3: &svc
        controller: llama3
        ports:
          http:
            port: 8080
            protocol: HTTP
            appProtocol: http
      # codellama:
      #   <<: *svc
      #   controller: codellama
      phi3:
        <<: *svc
        controller: phi3
      r1-qwen-32b:
        <<: *svc
        controller: r1-qwen-32b
    ingress:
      llama3:
        className: nginx-internal
        hosts:
          - host: &host "llama3.${DNS_SHORT}"
            paths: &paths
              - path: /
                pathType: Prefix
                service:
                  identifier: llama3
                  port: http
        tls:
          - hosts: [*host]
      # codellama:
      #   className: nginx-internal
      #   hosts:
      #     - host: &host "codellama.${DNS_SHORT}"
      #       paths: &paths
      #         - path: /
      #           pathType: Prefix
      #           service:
      #             identifier: codellama
      #             port: http
      #   tls:
      #     - hosts: [*host]
      phi3:
        className: nginx-internal
        hosts:
          - host: &host "phi3.${DNS_SHORT}"
            paths: &paths
              - path: /
                pathType: Prefix
                service:
                  identifier: phi3
                  port: http
        tls:
          - hosts: [*host]
      r1-qwen-32b:
        className: nginx-internal
        hosts:
          - host: &host "deepseek.${DNS_SHORT}"
            paths: &paths
              - path: /
                pathType: Prefix
                service:
                  identifier: r1-qwen-32b
                  port: http
        tls:
          - hosts: [*host]
    persistence:
      misc:
        existingClaim: mlc-llm-misc
        globalMounts:
          - subPath: data
            path: /data
      tmp:
        type: emptyDir
        globalMounts:
          - subPath: tmp
            path: /tmp # TODO: check why /tmp on CephFS breaks Git clone
    defaultPodOptions:
      automountServiceAccountToken: false
      enableServiceLinks: false
      hostAliases:
        - ip: "${APP_IP_AUTHENTIK:=127.0.0.1}"
          hostnames: ["${APP_DNS_AUTHENTIK:=authentik}"]
      hostUsers: false
      securityContext:
        runAsNonRoot: true
        runAsUser: &uid 65534
        runAsGroup: *uid
        fsGroup: *uid
        fsGroupChangePolicy: Always
        supplementalGroups: [44, 226] # iGPU
        seccompProfile: { type: "Unconfined" } # GPU hangs with RuntimeDefault
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: *app
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: fuckoff.home.arpa/mlc-llm
                    operator: DoesNotExist
    networkpolicies:
      from-frontends:
        podSelector: {}
        policyTypes: [Ingress, Egress]
        rules:
          ingress:
            - from:
                - namespaceSelector:
                    matchLabels:
                      kubernetes.io/metadata.name: open-webui
                - namespaceSelector:
                    matchLabels:
                      kubernetes.io/metadata.name: sillytavern
