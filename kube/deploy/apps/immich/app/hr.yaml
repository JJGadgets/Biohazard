---
apiVersion: helm.toolkit.fluxcd.io/v2beta2
kind: HelmRelease
metadata:
  name: &app immich
  namespace: *app
spec:
  interval: 5m
  chart:
    spec:
      chart: app-template
      version: 3.1.0
      sourceRef:
        name: bjw-s
        kind: HelmRepository
        namespace: flux-system
  values:
    controllers:
      immich:
        type: deployment
        replicas: 1
        pod:
          labels:
            ingress.home.arpa/nginx-internal: allow
            db.home.arpa/pg: pg-home
            prom.home.arpa/kps: allow
            authentik.home.arpa/https: allow
        containers:
          main:
            image: &img
              repository: ghcr.io/immich-app/immich-server
              tag: v1.112.1@sha256:c4e817f0eadbd9a6c2699cc884d5e7070428daec813c17db77d31fcca5b78ca6
            command: &cmd ["tini", "--", "node", "/usr/src/app/dist/main"]
            args: ["immich"]
            env: &env
              TZ: "${CONFIG_TZ}"
              LD_PRELOAD: /usr/lib/x86_64-linux-gnu/libmimalloc.so.2
              NODE_ENV: production
              LOG_LEVEL: verbose
              IMMICH_MEDIA_LOCATION: &pvc /data
              IMMICH_METRICS: "true"
              IMMICH_SERVER_URL: http://immich.immich.svc.cluster.local:3001
              IMMICH_MACHINE_LEARNING_URL: http://immich-ml.immich.svc.cluster.local:3003
              REDIS_HOSTNAME: immich-redis.immich.svc.cluster.local
              REDIS_PORT: "6379"
              DB_VECTOR_EXTENSION: pgvector # I couldn't really care less for worser machine learning, over half my library is screenshots
              DB_URL:
                valueFrom:
                  secretKeyRef:
                    name: pg-home-pguser-immich-fixed
                    key: pgbouncer-uri-sslmode
            envFrom: &ef
              - secretRef:
                  name: immich-secrets
            securityContext: &sc
              readOnlyRootFilesystem: true
              allowPrivilegeEscalation: false
              capabilities:
                drop: ["ALL"]
            resources:
              requests:
                cpu: "10m"
                memory: "128Mi"
              limits:
                cpu: "3000m"
                memory: "2Gi"
      microservices:
        type: deployment
        replicas: 3
        strategy: RollingUpdate
        rollingUpdate:
          unavailable: "90%"
        pod:
          labels:
            db.home.arpa/pg: pg-home
            prom.home.arpa/kps: allow
          securityContext:
            runAsNonRoot: true
            runAsUser: &uid ${APP_UID_IMMICH:=1000}
            runAsGroup: *uid
            fsGroup: *uid
            fsGroupChangePolicy: Always
            supplementalGroups: [44, 104, 109, 128, 226] # GPU
            seccompProfile: { type: "RuntimeDefault" }
          topologySpreadConstraints:
            - maxSkew: 1
              topologyKey: kubernetes.io/hostname
              whenUnsatisfiable: DoNotSchedule
              labelSelector:
                matchLabels:
                  app.kubernetes.io/name: *app
                  app.kubernetes.io/instance: *app
                  app.kubernetes.io/component: microservices
        containers:
          main:
            image: *img
            command: *cmd
            args: ["microservices"]
            env: *env
            securityContext: *sc
            resources:
              requests:
                cpu: "100m"
                memory: "300Mi"
                gpu.intel.com/i915: "1"
              limits:
                cpu: "1000m" # my machine will actually die
                memory: "2Gi"
                gpu.intel.com/i915: "1"
      ml:
        type: deployment
        replicas: 3
        strategy: RollingUpdate
        rollingUpdate:
          unavailable: "90%"
        pod:
          labels:
            db.home.arpa/pg: pg-home
          topologySpreadConstraints:
            - maxSkew: 1
              topologyKey: kubernetes.io/hostname
              whenUnsatisfiable: DoNotSchedule
              labelSelector:
                matchLabels:
                  app.kubernetes.io/name: *app
                  app.kubernetes.io/instance: *app
                  app.kubernetes.io/component: ml
        containers:
          main:
            image:
              repository: ghcr.io/immich-app/immich-machine-learning
              tag: v1.112.1@sha256:9600eff5a66ae426293f00b171711bc1647c85cf966d759ee08ab2d05e0580b5
            env: *env
            securityContext: *sc
            resources:
              requests:
                cpu: "10m"
                memory: "1Gi"
              limits:
                cpu: "1000m"
                memory: "6Gi"
      ml-model-pull:
        type: cronjob
        cronjob:
          schedule: "@daily"
          concurrencyPolicy: "Replace"
        pod:
          labels:
            egress.home.arpa/internet: allow
        containers:
          main:
            image:
              repository: ghcr.io/immich-app/immich-machine-learning
              tag: v1.112.1@sha256:9600eff5a66ae426293f00b171711bc1647c85cf966d759ee08ab2d05e0580b5
            command: ["tini", "--"]
            args: ["/bin/sh", "-c", "tini -s -g -- /bin/sh -c 'exec ./start.sh' & read -t 600 || kill -SIGINT $!"] # run for 10 minutes to pull models via preload, then kill process, hopefully it doesn't crash
            env:
              # default models as of v1.112.1
              MACHINE_LEARNING_PRELOAD__CLIP: ViT-B-32__openai
              MACHINE_LEARNING_PRELOAD__FACIAL_RECOGNITION: buffalo_l
            securityContext: *sc
            resources:
              requests:
                cpu: "10m"
              limits:
                cpu: "1000m"
                memory: "1Gi"
      redis:
        type: deployment
        replicas: 1
        containers:
          redis:
            image:
              repository: "public.ecr.aws/docker/library/redis"
              tag: "7.2.4@sha256:f14f42fc7e824b93c0e2fe3cdf42f68197ee0311c3d2e0235be37480b2e208e6"
            command: ["redis-server", "--save", "300 1", "--appendonly", "yes"] # save and appendonly options forcibly disable RDB and AOF persistence entirely
            securityContext: *sc
            resources:
              requests:
                cpu: "10m"
                memory: "32Mi"
              limits:
                cpu: "1000m"
                memory: "512Mi"
    service:
      immich:
        controller: immich
        ports:
          http:
            port: 3001
          metrics:
            port: 8081
      microservices:
        controller: microservices
        ports:
          metrics:
            port: 8081
      ml:
        controller: ml
        ports:
          http:
            port: 3003
      redis:
        controller: redis
        ports:
          http:
            port: 6379
    ingress:
      main:
        className: nginx-internal
        hosts:
          - host: &host "${APP_DNS_IMMICH:=immich}"
            paths: &paths
              - path: /
                pathType: Prefix
                service:
                  identifier: immich
                  port: http
        tls:
          - hosts: [*host]
    persistence:
      data:
        existingClaim: immich-data
        advancedMounts:
          immich: &mount
            main:
              - subPath: data
                path: *pvc
          microservices: *mount
          redis:
            main:
              - subPath: redis
                path: /data
      misc:
        existingClaim: immich-misc
        advancedMounts:
          immich: &misc
            main:
              - subPath: encodedvideo
                path: /data/encoded-video
              - subPath: thumbs
                path: /data/thumbs
          microservices: *misc
          ml:
            main:
              - &mlpvc
                subPath: ml-models-cache
                path: /cache
                readOnly: true
          ml-model-pull:
            main:
              - <<: *mlpvc
                readOnly: false
      tmp:
        type: emptyDir
        medium: Memory
        sizeLimit: 16Mi
        globalMounts:
          - subPath: tmp
            path: /tmp
          - subPath: geocode
            path: /usr/src/app/.reverse-geocoding-dump
          - subPath: geoname
            path: /usr/src/app/node_modules/local-reverse-geocoder/geonames_dump
          - subPath: transformers
            path: /usr/src/app/.transformers_cache
      pg-ca:
        type: secret
        name: pg-home-ca
        defaultMode: 0400
        globalMounts:
          - subPath: ca.crt
            path: /secrets/pg/ca.crt
    defaultPodOptions:
      automountServiceAccountToken: false
      enableServiceLinks: false
      hostAliases:
        - ip: "${APP_IP_AUTHENTIK:=127.0.0.1}"
          hostnames: ["${APP_DNS_AUTHENTIK:=authentik}"]
      securityContext:
        runAsNonRoot: true
        runAsUser: &uid ${APP_UID_IMMICH:=1000}
        runAsGroup: *uid
        fsGroup: *uid
        fsGroupChangePolicy: Always
        seccompProfile: { type: "RuntimeDefault" }
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: fuckoff.home.arpa/immich
                    operator: DoesNotExist
    networkpolicies:
      immich:
        podSelector: &sel
          matchExpressions:
            - key: app.kubernetes.io/name
              operator: In
              values: [immich]
            - key: app.kubernetes.io/instance
              operator: NotIn
              values: [ml-model-pull]
        policyTypes: [Ingress, Egress]
        rules:
          ingress:
            - from: [{podSelector: *sel}]
          egress:
            - to: [{podSelector: *sel}]
    serviceMonitor:
      immich:
        serviceName: immich
        endpoints:
          - port: metrics
            scheme: http
            path: /metrics
            interval: 1m
            scrapeTimeout: 30s
      microservices:
        serviceName: microservices
        endpoints:
          - port: metrics
            scheme: http
            path: /metrics
            interval: 1m
            scrapeTimeout: 30s
