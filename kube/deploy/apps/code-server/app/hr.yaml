---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: &app code-server
  namespace: *app
spec:
  interval: 5m
  chart:
    spec:
      chart: app-template
      version: "2.5.0"
      sourceRef:
        name: bjw-s
        kind: HelmRepository
        namespace: flux-system
  values:
    controllers:
      main:
        type: deployment
        replicas: 1
        pod:
          labels:
            tailscale.com/expose: "true"
            ingress.home.arpa/nginx-internal: "allow"
            egress.home.arpa/apiserver: "allow"
            egress.home.arpa/world: "allow"
        containers:
          main:
            image:
              repository: "ghcr.io/coder/code-server"
              tag: "4.20.1"
            command: ["dumb-init", "/usr/bin/code-server"]
            args:
              - "--auth"
              - "none"
              - "--disable-telemetry"
              - "--user-data-dir"
              - "/home/coder/.vscode"
              - "--extensions-dir"
              - "/home/coder/.vscode"
              - "--bind-addr"
              - "0.0.0.0:8080"
              - "--port"
              - &port "8080"
              - &dir "/home/coder"
            env:
              TZ: "${CONFIG_TZ}"
              SSH_AUTH_SOCK: ""
            securityContext: &sc
              readOnlyRootFilesystem: true
              allowPrivilegeEscalation: false
              capabilities:
                drop: ["ALL"]
            resources:
              requests:
                cpu: "10m"
              limits:
                cpu: "1000m" # I previously had a code-server that would eat cores
                memory: "1Gi"
          #nix:
          #  image:
          #    repository: "docker.io/nixos/nix"
          #    tag: "2.20.1@sha256:bbd436fac4b50712fb065c3cb1d74702aa9d731cc6cc702dbba20a9ccb2d8769"
          #  args: ["-c", "sleep infinity"]
          #  env:
          #    TZ: "${CONFIG_TZ}"
          #  securityContext: *sc
          #  resources:
          #    requests:
          #      cpu: "10m"
          #      memory: "256Mi"
          #    limits:
          #      cpu: "1000m" # I previously had a code-server that would eat cores
          #      memory: "1Gi"
    service:
      main:
        ports:
          http:
            port: *port
    ingress:
      main:
        enabled: true
        primary: true
        className: "tailscale"
        hosts:
          - host: &host "vs.${DNS_TS}"
            paths:
              - path: /
                pathType: Prefix
                service:
                  name: main
                  port: http
        tls:
          - hosts: [*host]
    persistence:
      config:
        enabled: true
        existingClaim: "code-server-data"
        globalMounts:
          - subPath: "data"
            path: *dir
      misc: # not backed up
        enabled: true
        existingClaim: "code-server-misc"
        globalMounts:
          - subpath: "brew"
            path: "/home/linuxbrew"
          - subpath: "nix"
            path: "/nix"
      ssh:
        enabled: true
        type: secret
        name: "code-server-ssh"
        defaultMode: 0600
        advancedMounts:
          main:
            main:
              - subPath: "privkey"
                path: "/home/coder/.ssh/id_rsa"
                readOnly: true
              - subPath: "pubkey"
                path: "/home/coder/.ssh/id_rsa.pub"
                readOnly: true
      tmp:
        enabled: true
        type: emptyDir
        medium: Memory
        globalMounts:
          - subPath: "tmp"
            path: "/tmp"
            readOnly: false
    defaultPodOptions:
      automountServiceAccountToken: false
      enableServiceLinks: false
      securityContext:
        runAsNonRoot: true
        runAsUser: &uid 1000
        runAsGroup: *uid
        fsGroup: *uid
        fsGroupChangePolicy: "Always"
        seccompProfile: { type: "RuntimeDefault" }
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: "kubernetes.io/hostname"
          whenUnsatisfiable: "DoNotSchedule"
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: *app
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: "fuckoff.home.arpa/code-server"
                    operator: "DoesNotExist"
