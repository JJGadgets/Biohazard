---
# yaml-language-server: $schema=https://raw.githubusercontent.com/bjw-s/helm-charts/common-3.7.3/charts/other/app-template/schemas/helmrelease-helm-v2.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app sillytavern
  namespace: *app
spec:
  interval: 5m
  chart:
    spec:
      chart: app-template
      version: 3.7.3
      sourceRef:
        name: bjw-s
        kind: HelmRepository
        namespace: flux-system
  values:
    controllers:
      sillytavern:
        type: deployment
        replicas: 1
        pod:
          labels:
            ingress.home.arpa/nginx-internal: allow
        containers:
          main:
            image: &img
              repository: ghcr.io/sillytavern/sillytavern
              tag: 1.12.8@sha256:2842e8317fe0d4cb6cd4034d52110904c2a30d88840c3879e8576e059a56e942
            env: &env
              TZ: "${CONFIG_TZ}"
            securityContext: &sc
              readOnlyRootFilesystem: true
              allowPrivilegeEscalation: false
              capabilities:
                drop: ["ALL"]
            resources:
              requests:
                cpu: "10m"
                memory: "128Mi"
              limits:
                cpu: "1"
                memory: "1Gi"
            probes:
              liveness:
                enabled: true
              readiness:
                enabled: true
              #startup:
              #  enabled: true
    service:
      sillytavern:
        controller: sillytavern
        ports:
          http:
            port: 8000
            protocol: HTTP
            appProtocol: http
    ingress:
      main:
        className: nginx-internal
        hosts:
          - host: &host "${APP_DNS_SILLYTAVERN:=sillytavern}"
            paths: &paths
              - path: /
                pathType: Prefix
                service:
                  identifier: sillytavern
                  port: http
        tls:
          - hosts: [*host]
    persistence:
      data:
        existingClaim: sillytavern-data
        globalMounts:
          - subPath: config
            path: /home/node/app/config
          - subPath: data
            path: /home/node/app/data
          - subPath: plugins
            path: /home/node/app/plugins
      tmp:
        type: emptyDir
        medium: Memory
        globalMounts:
          - subPath: tmp
            path: /tmp
          - subPath: backups # temporarily not using due to VolSync
            path: /home/node/app/backups
    defaultPodOptions:
      automountServiceAccountToken: false
      enableServiceLinks: false
      hostAliases:
        - ip: "${APP_IP_AUTHENTIK:=127.0.0.1}"
          hostnames: ["${APP_DNS_AUTHENTIK:=authentik}"]
      dnsConfig:
        options:
          - name: ndots
            value: "1"
      hostUsers: false
      securityContext:
        runAsNonRoot: true
        runAsUser: &uid 1000
        runAsGroup: *uid
        fsGroup: *uid
        fsGroupChangePolicy: Always
        seccompProfile: { type: "RuntimeDefault" }
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: *app
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: fuckoff.home.arpa/sillytavern
                    operator: DoesNotExist
    networkpolicies:
      to-llm:
        podSelector: {}
        policyTypes: [Ingress, Egress]
        rules:
          egress:
            - to:
                - namespaceSelector:
                    matchLabels:
                      kubernetes.io/metadata.name: mlc-llm
