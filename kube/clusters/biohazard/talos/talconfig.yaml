---
# yaml-language-server: $schema=https://raw.githubusercontent.com/budimanjojo/talhelper/master/pkg/config/schemas/talconfig.json
clusterName: biohazard
talosVersion: v1.9.1
kubernetesVersion: v1.30.1
endpoint: "https://c.${DNS_CLUSTER}:6443"
allowSchedulingOnMasters: true
allowSchedulingOnControlPlanes: true

cniConfig:
  name: none

clusterPodNets:
  - "${IP_POD_CIDR_V4}"
clusterSvcNets:
  - "${IP_SVC_CIDR_V4}"

additionalApiServerCertSans: &san
  - "${IP_CLUSTER_VIP}"
  - "${IP_ROUTER_VLAN_K8S}"
  - "c.${DNS_CLUSTER}"
  - "127.0.0.1" # KubePrism
  - "tailscale-operator-k8s"
  - "tailscale-operator-k8s.${DNS_TS}"

additionalMachineCertSans: *san

nodes:

  - &m720q
    hostname: "ange.${DNS_CLUSTER}" # M720q, i5-8500T 6C6T, 64GB RAM, 256GB OS NVMe
    ipAddress: "${IP_ROUTER_VLAN_K8S_PREFIX}1"
    controlPlane: true
    installDiskSelector:
      size: "<= 600GB"
      type: "nvme"
    nameservers: ["${IP_ROUTER_VLAN_K8S}"]
    disableSearchDomain: true
    networkInterfaces:
      - &m720q-net
        interface: br0
        mtu: 1500
        dhcp: false
        bridge:
          interfaces: [bond0]
          stp: {enabled: true}
        addresses: ["${IP_ROUTER_VLAN_K8S_PREFIX}1/28"]
        routes:
          - network: "${IP_ROUTER_VLAN_K8S_CIDR}"
            metric: 1
          - network: "0.0.0.0/0"
            gateway: "${IP_ROUTER_VLAN_K8S}"
        vip:
          ip: "${IP_CLUSTER_VIP}"
      - &m720q-bond0
        interface: bond0
        mtu: 1500
        bond: &bond0
          mode: active-backup
          miimon: 100
          primary: eno1
          deviceSelectors:
            # Onboard Intel 1GbE (eno1)
            - driver: e1000e
              physical: true
            # Mellanox ConnectX (enp1s0)
            - driver: "mlx4_core"
              physical: true
    machineSpec:
      secureboot: true
    schematic:
      customization:
        systemExtensions:
          officialExtensions:
            - siderolabs/gvisor
            - siderolabs/gvisor-debug
            - siderolabs/i915
            - siderolabs/intel-ucode
            - siderolabs/iscsi-tools
            - siderolabs/kata-containers
            - siderolabs/lldpd
    # patches:
    #   - |
    #     machine:
    #       sysfs:
    #         devices.system.cpu.intel_pstate.max_perf_pct: "80" # limit max frequency to 2.8GHz
    #         devices.system.cpu.intel_pstate.hwp_dynamic_boost: "1"

  - <<: *m720q
    hostname: "charlotte.${DNS_CLUSTER}" # M720q, i5-8500T 6C6T, 64GB RAM, 256GB OS NVMe, WiFi M.2 screw stuck LOL
    ipAddress: "${IP_ROUTER_VLAN_K8S_PREFIX}2"
    networkInterfaces:
      - <<: *m720q-net
        addresses: ["${IP_ROUTER_VLAN_K8S_PREFIX}2/28"]
      - *m720q-bond0

  - <<: *m720q # TODO: this is cp2 but bare metal, only apply after PVE converted to Talos
    hostname: "chise.${DNS_CLUSTER}" # M720q, i3-8100T 4C4T, 32GB RAM, 512GB OS NVMe
    ipAddress: "${IP_ROUTER_VLAN_K8S_PREFIX}3"
    networkInterfaces:
      - <<: *m720q-net
        addresses: ["${IP_ROUTER_VLAN_K8S_PREFIX}3/28"]
      - *m720q-bond0
    patches:
      - |
        machine:
          sysfs:
            devices.system.cpu.intel_pstate.max_perf_pct: "90" # limit max frequency to 2.8GHz
            devices.system.cpu.intel_pstate.hwp_dynamic_boost: "1"

  - hostname: "thunderscreech.${DNS_CLUSTER}" # R730xd Proxmox VM
    ipAddress: "${IP_ROUTER_VLAN_K8S_PREFIX}4"
    controlPlane: false
    installDisk: /dev/vda
    nameservers: ["${IP_ROUTER_VLAN_K8S}"]
    disableSearchDomain: true
    nodeLabels:
      node-role.kubernetes.io/vm: ""
      node-role.kubernetes.io/pve: ""
      node-role.kubernetes.io/worker: ""
    networkInterfaces:
      - interface: br0
        mtu: 1500
        dhcp: false
        bridge:
          interfaces: [bond0]
          stp: {enabled: true}
        addresses: ["${IP_ROUTER_VLAN_K8S_PREFIX}4/28"]
        routes:
          - network: "${IP_ROUTER_VLAN_K8S_CIDR}"
            metric: 1
          - network: "0.0.0.0/0"
            gateway: "${IP_ROUTER_VLAN_K8S}"
      - interface: bond0
        mtu: 1500
        bond:
          mode: active-backup
          miimon: 100
          deviceSelectors:
            # VirtIO NIC
            - driver: virtio_net
              physical: true
    machineSpec:
      secureboot: true
    schematic:
      customization:
        systemExtensions:
          officialExtensions:
            - siderolabs/gvisor
            - siderolabs/intel-ucode
            - siderolabs/iscsi-tools
            - siderolabs/lldpd
            # - siderolabs/nvidia-open-gpu-kernel-modules

patches:
  # # Cilium BGP set to active listen
  # - |-
  #   machine:
  #     nodeAnnotations:
  #       cilium.io/bgp-virtual-router.${ASN_ROUTER}="local-port=179"
  # set all disks to no scheduler
  - |-
    machine:
      udev:
        rules:
          # set all disks to `none` scheduler (optimal setting for Ceph and ZFS)
          - SUBSYSTEM=="block", ENV{DEVTYPE}=="disk", ATTR{queue/scheduler}="none"
          # allow GID 44 (video) to use Intel GPU
          #- SUBSYSTEM=="drm", KERNEL=="renderD*", GROUP="44", MODE="0660"
          - SUBSYSTEM=="drm", GROUP="44", MODE="0660"

  - &machinePatch |-
    machine:
      install:
        bootloader: true
      network:
        extraHostEntries:
          - ip: "${IP_CLUSTER_VIP}"
            aliases: ["c.${DNS_CLUSTER}"]
          - ip: "${IP_ROUTER_VLAN_K8S_PREFIX}1"
            aliases: ["c.${DNS_CLUSTER}"]
          - ip: "${IP_ROUTER_VLAN_K8S_PREFIX}2"
            aliases: ["c.${DNS_CLUSTER}"]
          - ip: "${IP_ROUTER_VLAN_K8S_PREFIX}3"
            aliases: ["c.${DNS_CLUSTER}"]
          - ip: "${IP_HERCULES}"
            aliases: ["hercules.mesh.cilium.io"]
          - ip: "${IP_TRUENAS}"
            aliases: ["nas.${DNS_MAIN}"]
      time:
        disabled: false
        servers: ["${IP_ROUTER_VLAN_K8S}"]
        bootTimeout: 2m0s
#      kernel:
#        modules:
#          - name: nct6683
#            parameters: ["force=on"]
#          - name: e1000e
#            parameters: ["Node=0"]

  - &LUKS |
    machine:
      systemDiskEncryption:
        ephemeral: &fde
          provider: luks2
          keys:
            - slot: 0
              tpm: {}
        state: *fde

  - &clusterPatch |-
    cluster:
      allowSchedulingOnMasters: true
      allowSchedulingOnControlPlanes: true
      discovery:
        enabled: true
        registries:
          kubernetes:
            disabled: false
          service:
            disabled: true
      proxy:
        disabled: true

  - &kubePrism |-
    machine:
      features:
        kubePrism:
          enabled: true
          port: 7445

  - &hostDNS |
    machine:
      features:
        hostDNS:
          enabled: true
          resolveMemberNames: true
          forwardKubeDNSToHost: false

  - &kubeletSubnet |-
    machine:
      kubelet:
        nodeIP:
          validSubnets:
            - "${IP_ROUTER_VLAN_K8S_CIDR}"

  - &kubeletConfig |-
    machine:
      kubelet:
        extraConfig:
          maxPods: 200

  # patch containerd for spegel (discard)
  - &spegel |
    machine:
      files:
        - op: create
          path: /etc/cri/conf.d/20-customization.part
          permissions: 0o644
          content: |
            [plugins."io.containerd.grpc.v1.cri"]
              enable_unprivileged_ports = true
              enable_unprivileged_icmp = true
            [plugins."io.containerd.grpc.v1.cri".containerd]
              discard_unpacked_layers = false

  - &nfsMountOptions |
    machine:
      files:
        - op: overwrite
          path: /etc/nfsmount.conf
          permissions: 420
          content: |
            [ NFSMount_Global_Options ]
            nfsvers=4.2
            hard=True
            noatime=True
            nodiratime=True
            rsize=131072
            wsize=131072
            nconnect=8

  - &kubeletLogs |
    machine:
      kubelet:
        extraMounts:
          - type: bind
            options: [bind, rw, exec, suid]
            source: /run/kubelet-logs-pods
            destination: /var/log/pods

controlPlane:
  patches:
    - &apiServerResources |-
      cluster:
        apiServer:
          resources:
            requests:
              cpu: 200m
              memory: 4Gi
            limits:
              memory: 4Gi

    - &apiServerLogs |
      cluster:
        apiServer:
          extraArgs:
            audit-log-path: "/dev/null"
            feature-gates: AuthorizeNodeWithSelectors=false
          auditPolicy:
            apiVersion: audit.k8s.io/v1
            kind: Policy
            rules:
              - level: None

    - &nodeCidrSize |-
      - op: add
        path: /cluster/controllerManager/extraArgs
        value:
          node-cidr-mask-size: 23

    - &etcdSubnet |-
      cluster:
        etcd:
          advertisedSubnets:
            - "${IP_ROUTER_VLAN_K8S_CIDR}"

    - &etcdQuota |-
      cluster:
        etcd:
          extraArgs:
            quota-backend-bytes: 4294967296 # 4 GiB
    # https://www.talos.dev/v1.5/advanced/etcd-maintenance/#space-quota
    # maximum recommended is 8GiB, will resize to 4GiB for now so etcd won't shoot its load all at once

    - &metrics |-
      cluster:
        etcd:
          extraArgs:
            listen-metrics-urls: "http://0.0.0.0:2381"

    - &scheduler |-
      cluster:
        scheduler:
          config:
            apiVersion: kubescheduler.config.k8s.io/v1
            kind: KubeSchedulerConfiguration
            profiles:
              - schedulerName: default-scheduler
                pluginConfig:
                  - name: PodTopologySpread
                    args:
                      defaultingType: List
                      defaultConstraints:
                        - maxSkew: 1
                          topologyKey: "kubernetes.io/hostname"
                          whenUnsatisfiable: DoNotSchedule
                        - maxSkew: 3
                          topologyKey: "topology.kubernetes.io/zone"
                          whenUnsatisfiable: ScheduleAnyway

    - &talosAPI |
      machine:
        features:
          kubernetesTalosAPIAccess:
            enabled: true
            allowedRoles:
              - os:admin
            allowedKubernetesNamespaces:
              - system-upgrade-controller
              - talos-backup
              - code-server
