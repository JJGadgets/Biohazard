---
# yaml-language-server: $schema=https://raw.githubusercontent.com/budimanjojo/talhelper/master/pkg/config/schemas/talconfig.json
clusterName: biohazard
talosVersion: v1.10.3
kubernetesVersion: v1.32.0
endpoint: "https://c.${DNS_CLUSTER}:6443"
allowSchedulingOnMasters: true
allowSchedulingOnControlPlanes: true

cniConfig:
  name: none

clusterPodNets:
  - "${IP_POD_CIDR_V4}"
clusterSvcNets:
  - "${IP_SVC_CIDR_V4}"

additionalApiServerCertSans: &san
  - "${IP_CLUSTER_VIP}"
  - "${IP_ROUTER_VLAN_K8S}"
  - "c.${DNS_CLUSTER}"
  - "127.0.0.1" # KubePrism
  - "tailscale-operator-k8s"
  - "tailscale-operator-k8s.${DNS_TS}"

additionalMachineCertSans: *san

nodes:

  - &m720q
    hostname: "ange.${DNS_CLUSTER}" # M720q, i5-8500T 6C6T, 64GB RAM, 256GB OS NVMe
    ipAddress: "${IP_ROUTER_VLAN_K8S_PREFIX}1"
    controlPlane: true
    installDiskSelector:
      size: "<= 600GB"
      type: "nvme"
    nodeLabels:
      cpu-scaler.internal/multiplier: "1"
    nameservers: ["${IP_ROUTER_VLAN_K8S}"]
    disableSearchDomain: true
    networkInterfaces:
      - &m720q-net
        interface: br0
        mtu: 1500
        dhcp: true # for other IPs, IPv6 and dynamic DHCP DNS
        bridge:
          interfaces: [bond0]
          stp: {enabled: true}
        addresses: ["${IP_ROUTER_VLAN_K8S_PREFIX}1/28"]
        routes: &routes
          - network: "${IP_ROUTER_VLAN_K8S_CIDR}"
            metric: 1
          - network: "${IP_ROUTER_VLAN_K8S_LEGACY_PVE_CIDR}"
          - network: "0.0.0.0/0"
            gateway: "${IP_ROUTER_VLAN_K8S}"
        vip:
          ip: "${IP_CLUSTER_VIP}"
      - &m720q-bond0
        interface: bond0
        mtu: 1500
        bond: &bond0
          mode: active-backup
          miimon: 100
          primary: eno1
          primaryReselect: better
          deviceSelectors:
            # Onboard Intel 1GbE (eno1)
            - driver: e1000e
              physical: true
            # Mellanox ConnectX (enp1s0)
            - driver: "mlx4_core"
              physical: true
    machineSpec:
      secureboot: true
    schematic:
      customization:
        extraKernelArgs: &kernelArgs
          # maybe KubeVirt PCIe passthrough?
          - intel_iommu=on
          - iommu=pt
          # enable AppArmor over SELinux
          - -selinux
          - apparmor=1
          - lsm=yama,loadpin,safesetid,integrity,bpf,apparmor,lockdown,landlock,capability # https://github.com/siderolabs/pkgs/blob/8c4603e90335b9aaf180b954ebc43f65dcb2b7b6/kernel/build/config-amd64#L6522 as of 1.10.2, remove SELinux
          # disable IMA (upstreamed as of Talos 1.11.0-alpha.1)
          - ima=off
          - -ima_template
          - -ima_appraise
          - -ima_hash
          # allow long iGPU compute processes for headless stuff like LLMs
          - i915.enable_hangcheck=0
          - i915.request_timeout_ms=600000
        systemExtensions:
          officialExtensions:
            - siderolabs/gvisor
            - siderolabs/gvisor-debug
            - siderolabs/i915
            - siderolabs/intel-ucode
            - siderolabs/iscsi-tools
            - siderolabs/kata-containers
            - siderolabs/lldpd
    extensionServices:
      - &lldpd
        name: lldpd
        configFiles:
          - mountPath: /usr/local/etc/lldpd/lldpd.conf
            content: |
              configure lldpd portidsubtype ifname
              configure system description "Talos Node"
    extraManifests:
      - ./watchdog.yaml
    # patches:
      # - |
      #   machine:
      #     sysfs:
      #       devices.system.cpu.intel_pstate.max_perf_pct: "80" # limit max frequency to 2.8GHz
      #       devices.system.cpu.intel_pstate.hwp_dynamic_boost: "1"

  - <<: *m720q
    hostname: "charlotte.${DNS_CLUSTER}" # M720q, i5-8500T 6C6T, 64GB RAM, 256GB OS NVMe, WiFi M.2 screw stuck LOL
    ipAddress: "${IP_ROUTER_VLAN_K8S_PREFIX}2"
    networkInterfaces:
      - <<: *m720q-net
        addresses: ["${IP_ROUTER_VLAN_K8S_PREFIX}2/28"]
      - *m720q-bond0

  - <<: *m720q
    hostname: "chise.${DNS_CLUSTER}" # M720q, i3-8100T 4C4T, 32GB RAM, 512GB OS NVMe
    ipAddress: "${IP_ROUTER_VLAN_K8S_PREFIX}3"
    networkInterfaces:
      - <<: *m720q-net
        addresses: ["${IP_ROUTER_VLAN_K8S_PREFIX}3/28"]
      - *m720q-bond0
    # patches:
    #   - |
    #     machine:
    #       sysfs:
    #         devices.system.cpu.intel_pstate.max_perf_pct: "90" # limit max frequency to 2.8GHz
    #         devices.system.cpu.intel_pstate.hwp_dynamic_boost: "1"

  - &ms01
    hostname: "dorothy.${DNS_CLUSTER}" # ms01, i5-8500T 6C6T, 64GB RAM, 256GB OS NVMe
    ipAddress: "${IP_ROUTER_VLAN_K8S_PREFIX}5"
    controlPlane: false
    installDiskSelector:
      # size: "<= 600GB"
      type: "nvme"
    nodeLabels:
      cpu-scaler.internal/multiplier: "0.5"
    nameservers: ["${IP_ROUTER_VLAN_K8S}"]
    disableSearchDomain: true
    networkInterfaces:
      - &ms01-net
        interface: br0
        mtu: 1500
        dhcp: true # for other IPs, IPv6 and dynamic DHCP DNS
        bridge:
          interfaces: [bond0]
          stp: {enabled: true}
        addresses: ["${IP_ROUTER_VLAN_K8S_PREFIX}5/28"]
        routes: *routes
        # vip:
        #   ip: "${IP_CLUSTER_VIP}"
      - &ms01-bond0
        interface: bond0
        mtu: 1500
        bond: &bond0
          mode: active-backup
          miimon: 100
          # primary: eno1
          primaryReselect: better
          deviceSelectors:
            # Onboard Intel i226-{LM|V} 2.5GbE
            - driver: igc
              physical: true
            # Intel X710 10Gbe
            - driver: i40e
              physical: true
    machineSpec:
      secureboot: true
    schematic:
      customization:
        extraKernelArgs: *kernelArgs
        systemExtensions:
          officialExtensions:
            - siderolabs/iscsi-tools
            - siderolabs/lldpd
            - siderolabs/kata-containers
            - siderolabs/gvisor
            - siderolabs/gvisor-debug
            - siderolabs/intel-ucode
            - siderolabs/i915
            - siderolabs/intel-ice-firmware
            - siderolabs/mei # Intel 12 gen & newer
            - siderolabs/thunderbolt
    extensionServices:
      - *lldpd
    extraManifests:
      - ./watchdog.yaml

  - hostname: "thunderscreech.${DNS_CLUSTER}" # R730xd Proxmox VM
    ipAddress: "${IP_ROUTER_VLAN_K8S_PREFIX}4"
    controlPlane: false
    installDisk: /dev/vda
    nameservers: ["${IP_ROUTER_VLAN_K8S}"]
    disableSearchDomain: true
    # nodeLabels: # no RBAC lol
    #   node-role.kubernetes.io/vm: ""
    #   node-role.kubernetes.io/pve: ""
    #   node-role.kubernetes.io/worker: ""
    networkInterfaces:
      - interface: br0
        mtu: 1500
        dhcp: false
        bridge:
          interfaces: [bond0]
          stp: {enabled: true}
        addresses: ["${IP_ROUTER_VLAN_K8S_PREFIX}4/28"]
        routes: *routes
      - interface: bond0
        mtu: 1500
        bond:
          mode: active-backup
          miimon: 100
          deviceSelectors:
            # VirtIO NIC
            - driver: virtio_net
              physical: true
    machineSpec:
      secureboot: true
    schematic:
      customization:
        systemExtensions:
          officialExtensions:
            - siderolabs/gvisor
            - siderolabs/intel-ucode
            - siderolabs/iscsi-tools
            - siderolabs/lldpd
            # - siderolabs/nvidia-open-gpu-kernel-modules
    extensionServices:
      - *lldpd

patches:
  # set all disks to no scheduler
  - |-
    machine:
      udev:
        rules:
          # set all disks to `none` scheduler (optimal setting for Ceph and ZFS)
          - SUBSYSTEM=="block", ENV{DEVTYPE}=="disk", KERNEL!="rbd*", ATTR{queue/scheduler}="none"
          # allow GID 44 (video) to use Intel GPU
          - SUBSYSTEM=="drm", GROUP="44", MODE="0660"

  - &hugepages |-
    machine:
      sysfs:
        kernel.mm.hugepages.hugepages-1048576kB.nr_hugepages: 4
        kernel.mm.hugepages.hugepages-2048kB.nr_hugepages: 1024

  - &machinePatch |-
    machine:
      install:
        bootloader: true
      network:
        extraHostEntries:
          - ip: "${IP_CLUSTER_VIP}"
            aliases: ["c.${DNS_CLUSTER}"]
          - ip: "${IP_ROUTER_VLAN_K8S_PREFIX}1"
            aliases: ["c.${DNS_CLUSTER}"]
          - ip: "${IP_ROUTER_VLAN_K8S_PREFIX}2"
            aliases: ["c.${DNS_CLUSTER}"]
          - ip: "${IP_ROUTER_VLAN_K8S_PREFIX}3"
            aliases: ["c.${DNS_CLUSTER}"]
          - ip: "${IP_HERCULES}"
            aliases: ["hercules.mesh.cilium.io"]
          - ip: "${IP_TRUENAS}"
            aliases: ["nas.${DNS_MAIN}"]
      time:
        disabled: false
        servers: ["${IP_ROUTER_VLAN_K8S}"]
        bootTimeout: 2m0s
      kernel:
        modules:
          - name: nbd # Ceph RBD CSI gets noisy without this, probably fine without though?
#          - name: nct6683
#            parameters: ["force=on"]
#          - name: e1000e
#            parameters: ["Node=0"]

  - &LUKS |
    machine:
      systemDiskEncryption:
        ephemeral: &fde
          provider: luks2
          keys:
            - slot: 0
              tpm: {}
        state: *fde

  - &clusterPatch |-
    cluster:
      allowSchedulingOnMasters: true
      allowSchedulingOnControlPlanes: true
      discovery:
        enabled: true
        registries:
          kubernetes:
            disabled: false
          service:
            disabled: true
      proxy:
        disabled: true

  - &kubePrism |-
    machine:
      features:
        kubePrism:
          enabled: true
          port: 7445

  - &hostDNS |
    machine:
      features:
        hostDNS:
          enabled: true
          resolveMemberNames: true
          forwardKubeDNSToHost: false

  - &kubeletSubnet |-
    machine:
      kubelet:
        nodeIP:
          validSubnets:
            - "${IP_ROUTER_VLAN_K8S_CIDR}"

  - &kubeletConfig |-
    machine:
      kubelet:
        extraConfig:
          maxPods: 200

  - &userNamespaces |-
    machine:
      sysctls:
        user.max_user_namespaces: "11255" # also allows gvisor
      kubelet:
        extraConfig:
          featureGates:
            UserNamespacesSupport: true
            UserNamespacesPodSecurityStandards: true

  - &PodLevelResources |-
    machine:
      kubelet:
        extraConfig:
          featureGates:
            PodLevelResources: true

  - &kubeletNodePressure |
    machine:
      kubelet:
        extraConfig:
          imageGCLowThresholdPercent: 70
          imageGCHighThresholdPercent: 90
          kubeReserved:
            cpu: 50m
            memory: 1Gi
            ephemeral-storage: 512Mi
          systemReserved:
            cpu: 50m
            memory: 1Gi
            ephemeral-storage: 512Mi
          evictionHard:
            nodefs.available: "5%"
          evictionMinimumReclaim:
            memory.available: "1Gi"
            nodefs.available: "1Gi"

  # patch containerd for spegel (discard)
  - &spegel |
    machine:
      files:
        - op: create
          path: /etc/cri/conf.d/20-customization.part
          permissions: 0o644
          content: |
            [plugins."io.containerd.cri.v1.images"]
              discard_unpacked_layers = false

  - &nfsMountOptions |
    machine:
      files:
        - op: overwrite
          path: /etc/nfsmount.conf
          permissions: 420
          content: |
            [ NFSMount_Global_Options ]
            nfsvers=4.2
            hard=True
            noatime=True
            nodiratime=True
            nconnect=8

  - &kubeletLogs |
    machine:
      kubelet:
        extraMounts:
          - type: bind
            options: [bind, rw, exec, suid]
            source: /run/kubelet-logs-pods
            destination: /var/log/pods

  - &netjank |
    machine:
      pods:
        - apiVersion: v1
          kind: Pod
          metadata:
            name: &name "net-jank"
            namespace: "kube-system"
          spec:
            hostNetwork: true
            hostPID: true
            restartPolicy: OnFailure
            containers:
              - &ct
                name: bridge-mac-set
                image: "public.ecr.aws/docker/library/alpine:latest"
                command: ["/bin/sh", "-c"]
                args:
                  - |
                    duration=600
                    endTime=$(( $(date +%s) + duration ))
                    while [ $(date +%s) -lt $$endTime ]; do
                      for br in $(brctl show | awk '!/bridge name/' | awk '/^[[:alnum:]]/{print $$1}'); do
                        ip link show dev $$br
                        ip link set $$br address $(brctl showmacs $$br | grep "0.00" | grep "  1" | grep "yes" | head -n 1 | awk '{print $$2}')
                        ip link show dev $$br
                      done
                      sleep 5
                    done
                securityContext:
                  privileged: true
                resources:
                  requests:
                    cpu: "0m"
                    memory: "0Mi"
                  limits:
                    cpu: "500m"
                    memory: "128Mi"
              - <<: *ct
                name: e1000e-fix
                args:
                  - |
                    find /sys/class/net/*/device/driver/module/drivers -maxdepth 1 -path "*e1000e*" | awk -F'/' '{print $$5}' | xargs -I% nsenter --mount=/host/proc/$(pidof /usr/local/bin/kubelet)/ns/mnt --net=/host/proc/$(pidof /usr/local/bin/kubelet)/ns/net -- sh -c "
                    echo '% - BEFORE' &&
                    echo '==========' &&
                    ethtool -k % &&
                    echo '==========' &&
                    echo 'Disabling offloads for %...' &&
                    ethtool -K % tso off gso off gro off &&
                    echo '==========' &&
                    echo '% - AFTER' &&
                    echo '==========' &&
                    ethtool -k % &&
                    echo '=========='"
                volumeMounts:
                  - name: netfs
                    mountPath: /host/net
                    readOnly: true
                  - name: procfs
                    mountPath: /host/proc
                    readOnly: true
            volumes:
              - name: netfs
                hostPath:
                  type: Directory
                  path: /sys
                  readOnly: true
              - name: procfs
                hostPath:
                  type: Directory
                  path: /proc
                  readOnly: true

controlPlane:
  patches:
    - &apiServerResources |-
      cluster:
        apiServer:
          resources:
            requests:
              cpu: 200m
              memory: 4Gi
            limits:
              memory: 4Gi

    - &apiServerLogs |
      cluster:
        apiServer:
          extraArgs:
            audit-log-path: "/dev/null" # disk health
          auditPolicy:
            apiVersion: audit.k8s.io/v1
            kind: Policy
            rules:
              - level: None

    - &nodeCidrSize |-
      - op: add
        path: /cluster/controllerManager/extraArgs
        value:
          node-cidr-mask-size: 23

    - &etcdSubnet |-
      cluster:
        etcd:
          advertisedSubnets:
            - "${IP_ROUTER_VLAN_K8S_CIDR}"

    - &etcdQuota |-
      cluster:
        etcd:
          extraArgs:
            quota-backend-bytes: 4294967296 # 4 GiB
    # https://www.talos.dev/v1.5/advanced/etcd-maintenance/#space-quota
    # maximum recommended is 8GiB, will resize to 4GiB for now so etcd won't shoot its load all at once

    - &metrics |-
      cluster:
        etcd:
          extraArgs:
            listen-metrics-urls: "http://0.0.0.0:2381"

    - &scheduler |-
      cluster:
        scheduler:
          config:
            apiVersion: kubescheduler.config.k8s.io/v1
            kind: KubeSchedulerConfiguration
            profiles:
              - schedulerName: default-scheduler
                pluginConfig:
                  - name: PodTopologySpread
                    args:
                      defaultingType: List
                      defaultConstraints:
                        - maxSkew: 1
                          topologyKey: "kubernetes.io/hostname"
                          whenUnsatisfiable: DoNotSchedule
                        - maxSkew: 3
                          topologyKey: "topology.kubernetes.io/zone"
                          whenUnsatisfiable: ScheduleAnyway

    - &talosAPI |
      machine:
        features:
          kubernetesTalosAPIAccess:
            enabled: true
            allowedRoles:
              - os:admin
              - os:operator
            allowedKubernetesNamespaces:
              - system-upgrade-controller
              - talos-backup
              - code-server
              - talosctl-image-pull-agent

    - &MutatingAdmissionPolicy |
      cluster:
        apiServer:
          extraArgs:
            runtime-config: admissionregistration.k8s.io/v1alpha1=true

    - &PodLevelResourcesCluster |
      cluster:
        apiServer:
          extraArgs:
            feature-gates: AuthorizeNodeWithSelectors=false,UserNamespacesSupport=true,UserNamespacesPodSecurityStandards=true,PodLevelResources=true,MutatingAdmissionPolicy=true # K8s 1.32 authz breaks Talos node discovery via Kubernetes, K8s 1.32+ user namespaces, K8s 1.32+ pod level resources, K8s 1.32+ mutating admission policy to avoid Kyverno
        controllerManager:
          extraArgs:
            feature-gates: PodLevelResources=true
        scheduler:
          extraArgs:
            feature-gates: PodLevelResources=true
