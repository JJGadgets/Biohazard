---
clusterName: biohazard
talosVersion: v1.5.4
kubernetesVersion: v1.28.2
endpoint: "https://c.${DNS_CLUSTER}:6443"
allowSchedulingOnMasters: true
dnsDomain: cluster.local
talosImageURL: "factory.talos.dev/installer/b585b09d94017b3086ee1bf52b422ca79abd60478fb7591bf21108f065769e7e"
# image extensions: amd-ucode, intel-ucode, i915-ucode, iscsi-tools, zfs
# install Tailscale later, service won't be considered up until Tailscale is enrolled (I think)

cniConfig:
  name: none

clusterPodNets:
  - "${IP_POD_CIDR_V4}"
clusterSvcNets:
  - "${IP_SVC_CIDR_V4}"

additionalApiServerCertSans:
  - "${IP_CLUSTER_VIP}"
  - "${IP_ROUTER_VLAN_K8S}"
  - "c.${DNS_CLUSTER}"

additionalMachineCertSans:
  - "${IP_CLUSTER_VIP}"
  - "${IP_ROUTER_VLAN_K8S}"
  - "c.${DNS_CLUSTER}"

nodes:

  - hostname: "cp1.${DNS_CLUSTER}"
    ipAddress: "${IP_ROUTER_VLAN_K8S_PREFIX}1"
    controlPlane: true
    installDisk: /dev/vda
    nameservers:
      - "${IP_HOME_DNS}"
    disableSearchDomain: true
    networkInterfaces:
      - interface: eth0
        mtu: 1500
        dhcp: false
        addresses:
          - "${IP_ROUTER_VLAN_K8S_PREFIX}1/28"
        routes:
          - network: "${IP_ROUTER_VLAN_K8S_CIDR}"
            metric: 1
          - network: "0.0.0.0/0"
            gateway: "${IP_ROUTER_VLAN_K8S}"
        vip:
          ip: "${IP_CLUSTER_VIP}"
      - interface: eth1
        # mtu: 9000 # PVE can't apply MTU 9000 because bridge set to MTU 1500 even though Ceph VLAN set to MTU 9000 LOL
        mtu: 1500
        dncp: false
        addresses: ["${IP_PVE_CEPH_PREFIX}4/28"]
        routes:
          - network: "${IP_PVE_CEPH_CIDR}"
            metric: 1

  - hostname: "cp2.${DNS_CLUSTER}"
    ipAddress: "${IP_ROUTER_VLAN_K8S_PREFIX}2"
    controlPlane: false
    installDisk: /dev/vda
    nameservers:
      - "${IP_HOME_DNS}"
    disableSearchDomain: true
    networkInterfaces:
      - interface: eth0
        mtu: 1500
        dhcp: false
        addresses:
          - "${IP_ROUTER_VLAN_K8S_PREFIX}2/28"
        routes:
          - network: "${IP_ROUTER_VLAN_K8S_CIDR}"
            metric: 1
          - network: "0.0.0.0/0"
            gateway: "${IP_ROUTER_VLAN_K8S}"
        # vip:
        #   ip: "${IP_CLUSTER_VIP}"
      - interface: eth1
        # mtu: 9000 # PVE can't apply MTU 9000 because bridge set to MTU 1500 even though Ceph VLAN set to MTU 9000 LOL
        mtu: 1500
        dncp: false
        addresses: ["${IP_PVE_CEPH_PREFIX}5/28"]
        routes:
          - network: "${IP_PVE_CEPH_CIDR}"
            metric: 1
    patches:
      # required for Talos to initialize i915 VFIO devices
      - &i915 |-
        machine:
          install:
            extensions:
              - image: ghcr.io/siderolabs/i915-ucode:20230310

  - hostname: "cp3.${DNS_CLUSTER}"
    ipAddress: "${IP_ROUTER_VLAN_K8S_PREFIX}3"
    controlPlane: true
    installDisk: /dev/vda
    nameservers:
      - "${IP_HOME_DNS}"
    disableSearchDomain: true
    networkInterfaces:
      - interface: eth0
        mtu: 1500
        dhcp: false
        addresses: ["${IP_ROUTER_VLAN_K8S_PREFIX}3/28"]
        routes:
          - network: "${IP_ROUTER_VLAN_K8S_CIDR}"
            metric: 1
          - network: "0.0.0.0/0"
            gateway: "${IP_ROUTER_VLAN_K8S}"
        vip:
          ip: "${IP_CLUSTER_VIP}"
      - interface: eth1
        # mtu: 9000 # PVE can't apply MTU 9000 because bridge set to MTU 1500 even though Ceph VLAN set to MTU 9000 LOL
        mtu: 1500
        dncp: false
        addresses: ["${IP_PVE_CEPH_PREFIX}6/28"]
        routes:
          - network: "${IP_PVE_CEPH_CIDR}"
            metric: 1

  - hostname: "blackfish.${DNS_CLUSTER}"
    ipAddress: "${IP_ROUTER_VLAN_K8S_PREFIX}4"
    controlPlane: true
    kernelModules:
      - name: "zfs"
    installDiskSelector:
      size: "<= 600GB"
      type: "ssd"
    nodeLabels:
      role.nodes.home.arpa/nas: "true"
    nameservers:
      - "${IP_HOME_DNS}"
    disableSearchDomain: true
    networkInterfaces:
      - mtu: 9000
        dhcp: false
        deviceSelector:
          driver: "mlx4_core"
          hardwareAddr: "*:6a"
        vlans:
          - vlanId: 58
            mtu: 1500
            dhcp: false
            addresses: ["${IP_ROUTER_VLAN_K8S_PREFIX}4/28"]
            routes:
              - network: "${IP_ROUTER_VLAN_K8S_CIDR}"
                metric: 1
              - network: "0.0.0.0/0"
                gateway: "${IP_ROUTER_VLAN_K8S}"
            vip:
              ip: "${IP_CLUSTER_VIP}"
          - vlanId: 678 # for PVE Ceph, still unsure if separate VLAN + Multus should be used for Rook-managed Ceph cluster, especially with netpols in question
            mtu: 9000
            dncp: false
            addresses: ["${IP_PVE_CEPH_PREFIX}7/28"]
            routes:
              - network: "${IP_PVE_CEPH_CIDR}"
                metric: 1

controlPlane:
  patches:
    - &kubeletExtraArgs |-
      - op: add
        path: /machine/kubelet/extraArgs
        value:
          feature-gates: CronJobTimeZone=true,GracefulNodeShutdown=true,ServerSideApply=true
    - &apiServerExtraArgs |-
      - op: add
        path: /cluster/apiServer/extraArgs
        value:
          feature-gates: CronJobTimeZone=true,GracefulNodeShutdown=true,ServerSideApply=true
    # - |-
    #   - op: add
    #     path: /cluster/controllerManager/extraArgs
    #     value:
    #       node-cidr-mask-size: 22
    - &machinePatch |-
      machine:
        install:
          bootloader: true
        network:
          extraHostEntries:
            - ip: "${IP_CLUSTER_VIP}"
              aliases:
                - "c.${DNS_CLUSTER}"
        time:
          disabled: false
          servers:
            - "${IP_ROUTER_LAN}"
          bootTimeout: 2m0s

    - &kubeletSubnet |-
      machine:
        kubelet:
          nodeIP:
            validSubnets:
              - "${IP_ROUTER_VLAN_K8S_CIDR}"

    - &etcdSubnet |-
      cluster:
        etcd:
          advertisedSubnets:
            - "${IP_ROUTER_VLAN_K8S_CIDR}"

    - &clusterPatch |-
      cluster:
        allowSchedulingOnMasters: true
        allowSchedulingOnControlPlanes: true
        discovery:
          enabled: true
          registries:
            kubernetes:
              disabled: false
            service:
              disabled: true
        proxy:
          disabled: true

    # - &scheduler |-
    #   cluster:
    #     scheduler:
    #       extraArgs:
    #         config: "/custom/etc/kube-scheduler/config.yaml"
    #       extraVolumes:
    #         - hostPath: "/var/etc/kube-scheduler"
    #           mountPath: "/custom/etc/kube-scheduler"
    #           readonly: true
    #   machine:
    #     files:
    #       - op: create
    #         path: "/var/etc/kube-scheduler/config.yaml"
    #         permissions: 0o400
    #         content: |
    #           apiVersion: kubescheduler.config.k8s.io/v1
    #           kind: KubeSchedulerConfiguration
    #           profiles:
    #             - schedulerName: default-scheduler
    #               pluginConfig:
    #                 - name: PodTopologySpread
    #                   args:
    #                     defaultingType: List
    #                     defaultConstraints:
    #                       - maxSkew: 1
    #                         topologyKey: "kubernetes.io/hostname"
    #                         whenUnsatisfiable: ScheduleAnyway
    #                       - maxSkew: 5
    #                         topologyKey: "topology.kubernetes.io/zone"
    #                         whenUnsatisfiable: ScheduleAnyway

    # Rook Ceph encrypted OSDs
    # TODO: https://github.com/siderolabs/talos/issues/3129
    - &encryptedOSD |-
      machine:
        files:
          - op: overwrite
            path: /etc/lvm/lvm.conf
            permissions: 0o644
            content: |
              backup {
                      backup = 0
                      archive = 0
              }

worker:
  patches:
    - *kubeletExtraArgs
    - *machinePatch
    - *clusterPatch
    - *kubeletSubnet
    # - *scheduler
    # Rook Ceph encrypted OSDs
    # TODO: https://github.com/siderolabs/talos/issues/3129
    - *encryptedOSD