---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: &app ${APPNAME}
  namespace: *app
spec:
  chart:
    spec:
      chart: app-template
      version: 1.5.1
      sourceRef:
        name: bjw-s
        kind: HelmRepository
        namespace: flux-system
  values:
    global:
      fullnameOverride: *app
    automountServiceAccountToken: false
    controller:
      # type: statefulset
      type: deployment
      replicas: 1
    image:
      repository: docker.io/${APPNAME}/server
      tag: latest@sha256:c10a2938d3a8c15169a3ed2f6d08d25430d22cef3d5749d57ab3a9052d60354c
    podLabels:
      ingress.home.arpa/nginx: "allow"
      db.home.arpa/pg: "pg-default"
      s3.home.arpa/store: "rgw-${CLUSTER_NAME}"
    env:
      TZ: "${CONFIG_TZ}"
    service:
      main:
        enabled: true
        type: LoadBalancer
        # eTP can be Cluster (for HA & failover) instead of Local since Cilium is configured in DSR mode, so proper source IP will still work
        externalTrafficPolicy: Cluster
        annotations:
          coredns.io/hostname: "${APP_DNS_APPNAME}"
          "io.cilium/lb-ipam-ips": "${APP_IP_APPNAME}"
        ports:
          http:
            enabled: true
            port: 443
            targetPort: 8443
            protocol: HTTPS
          ldap-tcp:
            enabled: true
            port: 636
            targetPort: 3636
            protocol: TCP
          ldap-udp:
            enabled: true
            port: 636
            targetPort: 3636
            protocol: UDP
    ingress:
      main:
        enabled: true
        primary: true
        ingressClassName: nginx
        annotations:
          external-dns.alpha.kubernetes.io/target: "${DNS_SHORT_CF}"
          external-dns.alpha.kubernetes.io/cloudflare-proxied: "true"
          nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
          # https://github.com/kubernetes/ingress-nginx/issues/6728
          nginx.ingress.kubernetes.io/server-snippet: |
            proxy_ssl_name ${APP_DNS_APPNAME};
            proxy_ssl_server_name on;
            large_client_header_buffers 4 8k;
            client_header_buffer_size 8k;
          # without header buffer size, will get following errors due to hardening ingress-nginx number of header buffers to 2 and header buffer size to 1k:
          # HTTP1.1 /v1/auth/valid: 400 Request Header Or Cookie Too Large
          # HTTP2 /v1/auth/valid: HTTP/2 stream was not closed cleanly before end of the underlying stream
        hosts:
          - host: &host "${APP_DNS_APPNAME}"
            paths:
              - path: /
                pathType: Prefix
        tls:
          - hosts:
              - *host
#     dnsConfig:
#       options:
#         - name: ndots
#           value: "1"
    podSecurityContext:
      runAsUser: &uid ${APP_UID_APPNAME}
      runAsGroup: *uid
      fsGroup: *uid
      fsGroupChangePolicy: Always
    volumeClaimTemplates:
      - name: data
        mountPath: /data
        accessMode: ReadWriteOnce
        size: 20Gi
        storageClass: block
      - name: backup
        mountPath: /backup
        accessMode: ReadWriteOnce
        size: 20Gi
        storageClass: block
    persistence:
      config:
        enabled: true
        type: configMap
        name: ${APPNAME}-config
        subPath: server.toml
        mountPath: /data/server.toml
        readOnly: true
      tls-fullchain:
        enabled: true
        type: secret
        name: ${APPNAME}-tls
        subPath: tls.crt
        mountPath: /tls/fullchain.pem
        readOnly: true
      tls-privkey:
        enabled: true
        type: secret
        name: ${APPNAME}-tls
        subPath: tls.key
        mountPath: /tls/privkey.pem
        readOnly: true
    configMaps:
      config:
        enabled: true
        data:
          server.toml: |-
            domain = "${APP_DNS_APPNAME}"
            origin = "https://${APP_DNS_APPNAME}"
            tls_chain = "/tls/fullchain.pem"
            tls_key = "/tls/privkey.pem"
            role = "WriteReplica"
            log_level = "verbose"
            bindaddress = "[::]:8443"
            ldapbindaddress = "[::]:3636"
            trust_x_forward_for = true
            db_path = "/data/${APPNAME}.db"
            db_fs_type = "other"
            [online_backup]
            path = "/backup/"
            schedule = "0 0 22 * * * *"
            versions = 7
    resources:
      requests:
        cpu: 10m
        memory: 128Mi
      limits:
        memory: 6000Mi
    initContainers:
      01-init-${APPNAME}-admin-password:
        command:
        - /bin/sh
        - -c
        - '[ -s /data/${APPNAME}.db ] || /sbin/${APPNAME}d recover_account -c /data/server.toml admin'
        image: docker.io/${APPNAME}/server:latest@sha256:c10a2938d3a8c15169a3ed2f6d08d25430d22cef3d5749d57ab3a9052d60354c
        imagePullPolicy: IfNotPresent
        volumeMounts:
        - mountPath: /data
          name: data
        - mountPath: /config
          name: config
      01-init-db:
        image: ghcr.io/onedr0p/postgres-init:14.8@sha256:d8391076d2c6449927a6409c4e72aaa5607c95be51969036f4feeb7c999638ea
        imagePullPolicy: IfNotPresent
        envFrom:
          - secretRef:
              name: gotosocial-pg
          - secretRef:
              name: gotosocial-pg-superuser
