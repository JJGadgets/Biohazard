---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: &app ${APPNAME}
  namespace: *app
spec:
  chart:
    spec:
      chart: app-template
      version: 2.0.2
      sourceRef:
        name: bjw-s
        kind: HelmRepository
        namespace: flux-system
  values:
    controllers:
      main:
        # type: statefulset
        type: deployment
        replicas: 1
        pod:
          labels:
            ingress.home.arpa/nginx: "allow"
            db.home.arpa/pg: "pg-default"
            s3.home.arpa/store: "rgw-${CLUSTER_NAME}"
        containers:
          main:
            image:
              repository: "docker.io/${APPNAME}/server"
              tag: "v"
            env:
              TZ: "${CONFIG_TZ}"
              GTS_STORAGE_S3_ACCESS_KEY:
                valueFrom:
                  secretKeyRef:
                    name: "${APPNAME}-data-s3"
                    key: "AWS_ACCESS_KEY_ID"
              GTS_STORAGE_S3_SECRET_KEY:
                valueFrom:
                  secretKeyRef:
                    name: "${APPNAME}-media-s3"
                    key: "AWS_SECRET_ACCESS_KEY"
            envFrom:
              - secretRef:
                  name: "${APPNAME}-secrets"
            resources:
              requests:
                cpu: 10m
                memory: 128Mi
              limits:
                memory: 6000Mi
        statefulset:
          volumeClaimTemplates:
            - name: data
              mountPath: "/data"
              accessMode: ReadWriteOnce
              size: 20Gi
              storageClass: block
            - name: backup
              mountPath: "/backup"
              accessMode: ReadWriteOnce
              size: 20Gi
              storageClass: block
        initContainers:
          01-init-${APPNAME}-admin-password:
            command:
            - /bin/sh
            - -c
            - '[ -s /data/${APPNAME}.db ] || /sbin/${APPNAME}d recover_account -c /data/server.toml admin'
            image: docker.io/${APPNAME}/server:latest
            imagePullPolicy: IfNotPresent
            volumeMounts:
            - mountPath: /data
              name: data
            - mountPath: /config
              name: config
          01-init-db:
            image: "ghcr.io/onedr0p/postgres-init:14.8"
            imagePullPolicy: IfNotPresent
            envFrom: [secretRef: {name: "${APPNAME}-pg-superuser"}]
    service:
      main:
        ports:
          http:
            port: 8080
      ssh:
        enabled: true
        primary: false
        controller: main
        type: LoadBalancer
        # eTP can be Cluster (for HA & failover) instead of Local since Cilium is configured in DSR mode, so proper source IP will still work
        externalTrafficPolicy: Cluster
        annotations:
          coredns.io/hostname: "${APP_DNS_APPNAME}"
          "io.cilium/lb-ipam-ips": "${APP_IP_APPNAME}"
        ports:
          http:
            enabled: true
            port: 443
            targetPort: 8443
            protocol: HTTPS
          ldap-tcp:
            enabled: true
            port: 636
            targetPort: 3636
            protocol: TCP
          ldap-udp:
            enabled: true
            port: 636
            targetPort: 3636
            protocol: UDP
    ingress:
      main:
        enabled: true
        primary: true
        className: nginx
        annotations:
          external-dns.alpha.kubernetes.io/target: "${DNS_SHORT_CF}"
          external-dns.alpha.kubernetes.io/cloudflare-proxied: "true"
          nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
          # https://github.com/kubernetes/ingress-nginx/issues/6728
          nginx.ingress.kubernetes.io/server-snippet: |
            proxy_ssl_name ${APP_DNS_APPNAME};
            proxy_ssl_server_name on;
            large_client_header_buffers 4 8k;
            client_header_buffer_size 8k;
          # without header buffer size, will get following errors due to hardening ingress-nginx number of header buffers to 2 and header buffer size to 1k:
          # HTTP1.1 /v1/auth/valid: 400 Request Header Or Cookie Too Large
          # HTTP2 /v1/auth/valid: HTTP/2 stream was not closed cleanly before end of the underlying stream
        hosts:
          - host: &host "${APP_DNS_APPNAME}"
            paths:
              - path: /
                pathType: Prefix
                service:
                  name: main
                  port: http
        tls:
          - hosts:
              - *host
#     dnsConfig:
#       options:
#         - name: ndots
#           value: "1"
    persistence:
      config:
        enabled: true
        type: configMap
        name: "${APPNAME}-config"
        advancedMounts:
          main:
            main:
              - subPath: "server.toml"
                mountPath: "/data/server.toml"
                readOnly: true
      data:
        enabled: true
        existingClaim: "${APPNAME}-data"
        advancedMounts:
          main:
            main:
              - path: /data
      nfs:
        enabled: true
        type: nfs
        server: "${IP_TRUENAS}"
        path: "${PATH_NAS_PERSIST_K8S}/${APPNAME}"
        advancedMounts:
          main:
            main:
              - path: /nfs
      tls:
        enabled: true
        type: secret
        name: "${APPNAME}-tls"
        defaultMode: 0400
        advancedMounts:
          main:
            main:
              - subPath: "tls.crt"
                path: "/tls/fullchain.pem"
                readOnly: true
              - subPath: "tls.key"
                path: "/tls/privkey.pem"
                readOnly: true
    configMaps:
      config:
        enabled: true
        data:
          server.toml: |-
            domain = "${APP_DNS_APPNAME}"
            origin = "https://${APP_DNS_APPNAME}"
            tls_chain = "/tls/fullchain.pem"
            tls_key = "/tls/privkey.pem"
            role = "WriteReplica"
            log_level = "verbose"
            bindaddress = "[::]:8443"
            ldapbindaddress = "[::]:3636"
            trust_x_forward_for = true
            db_path = "/data/${APPNAME}.db"
            db_fs_type = "other"
            [online_backup]
            path = "/backup/"
            schedule = "0 0 22 * * * *"
            versions = 7
    defaultPodOptions:
      automountServiceAccountToken: false
      securityContext:
        runAsUser: &uid ${APP_UID_APPNAME}
        runAsGroup: *uid
        fsGroup: *uid
        fsGroupChangePolicy: Always