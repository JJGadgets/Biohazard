---
# yaml-language-server: $schema=https://raw.githubusercontent.com/bjw-s/helm-charts/app-template-4.1.2/charts/other/app-template/schemas/helmrelease-helm-v2.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app ${APPNAME}
  namespace: *app
spec:
  interval: 5m
  chart:
    spec:
      chart: app-template
      version: 4.1.2
      sourceRef:
        name: bjw-s
        kind: HelmRepository
        namespace: flux-system
  values:
    controllers:
      app:
        # type: statefulset
        type: deployment
        replicas: 1
        pod:
          labels:
            ingress.home.arpa/nginx-internal: allow
            db.home.arpa/pg: pg-default
            s3.home.arpa/store: "rgw-${CLUSTER_NAME}"
            # authentik.home.arpa/https: allow
            # prom.home.arpa/kps: allow
            # egress.home.arpa/internet: allow
        containers:
          app:
            image: &img
              repository: ${IMAGENAME}
              tag: ${IMAGETAG}
            env: &env
              TZ: "${CONFIG_TZ}"
              _APPNAME_DATABASE_SOURCE:
                valueFrom:
                  secretKeyRef:
                    name: pg-default-pguser-${APPNAME}
                    key: pgbouncer-uri
              GTS_STORAGE_S3_ACCESS_KEY:
                valueFrom:
                  secretKeyRef:
                    name: ${APPNAME}-data-s3
                    key: AWS_ACCESS_KEY_ID
              GTS_STORAGE_S3_SECRET_KEY:
                valueFrom:
                  secretKeyRef:
                    name: ${APPNAME}-media-s3
                    key: AWS_SECRET_ACCESS_KEY
            envFrom: &envFrom
              - secretRef:
                  name: ${APPNAME}-secrets
            securityContext: &sc
              readOnlyRootFilesystem: true
              allowPrivilegeEscalation: false
              capabilities:
                drop: ["ALL"]
            resources:
              requests:
                cpu: "10m"
              limits:
                cpu: "1"
                memory: "512Mi"
            probes:
              liveness:
                enabled: true
              readiness:
                enabled: true
              # startup:
              #   enabled: true
              #   spec:
              #     periodSeconds: 1
              #     failureThreshold: 300
              #     initialDelaySeconds: 5
        # statefulset:
        #   volumeClaimTemplates:
        #     - name: data
        #       size: 20Gi
        #       storageClass: block
        #       accessMode: ReadWriteOnce
        #       advancedMounts:
        #         main: # only container name here
        #           - path: /data
        #         01-init-${APPNAME}-admin-password:
        #           - path: /data
        #     - name: backup
        #       accessMode: ReadWriteOnce
        #       size: 20Gi
        #       storageClass: block
        #       globalMounts:
        #         - path: /backup
        # initContainers:
        #   01-init-${APPNAME}-admin-password:
        #     image: *img
        #     command:
        #       - /bin/sh
        #       - -c
        #       - "[ -s /data/${APPNAME}.db ] || /sbin/${APPNAME}d recover_account -c /data/server.toml admin"
        #     securityContext: *sc
        #     #resources:
        #   01-init-db:
        #     image:
        #       repository: "ghcr.io/onedr0p/postgres-init"
        #       tag: "17.4@sha256:43dd04e91e861cf912378bad987afa168fa4f13d05528304907ad0aa351195d6"
        #     envFrom: [secretRef: { name: "${APPNAME}-pg-superuser" }]
        #     securityContext: *sc
        #     #resources:
    service:
      app:
        controller: app
        ports:
          http:
            port: 80
            targetPort: 8080
            protocol: HTTP
            appProtocol: http
      expose:
        primary: false
        controller: app
        type: LoadBalancer
        annotations:
          coredns.io/hostname: "${APP_DNS_APPNAME:=${APPNAME}}"
          lbipam.cilium.io/ips: "${APP_IP_APPNAME:=127.0.0.1}"
        ports:
          http:
            port: 443
            targetPort: 8443
            protocol: HTTPS
          ldap-tcp:
            port: 636
            targetPort: 3636
            protocol: TCP
          ldap-udp:
            port: 636
            targetPort: 3636
            protocol: UDP
    ingress:
      app:
        className: nginx-internal
        annotations:
          nginx.ingress.kubernetes.io/whitelist-source-range: "${IP_JJ_V4:=127.0.0.1/32}"
          external-dns.alpha.kubernetes.io/target: "${DNS_CF:=cf}"
          external-dns.alpha.kubernetes.io/cloudflare-proxied: "true"
          nginx.ingress.kubernetes.io/backend-protocol: HTTPS
          # https://github.com/kubernetes/ingress-nginx/issues/6728
          nginx.ingress.kubernetes.io/server-snippet: |
            proxy_ssl_name ${APP_DNS_APPNAME};
            proxy_ssl_server_name on;
          # without header buffer size, will get following errors due to hardening ingress-nginx number of header buffers to 2 and header buffer size to 1k:
          # HTTP1.1 /v1/auth/valid: 400 Request Header Or Cookie Too Large
          # HTTP2 /v1/auth/valid: HTTP/2 stream was not closed cleanly before end of the underlying stream
        hosts:
          - host: &host "${APP_DNS_APPNAME:=${APPNAME}}"
            paths: &paths
              - path: /
                pathType: Prefix
                service:
                  identifier: app
                  port: http
        tls:
          - hosts: [*host]
    persistence:
      config:
        type: configMap
        identifier: config
        advancedMounts:
          ${APPNAME}:
            main:
              - subPath: server.toml
                path: /data/server.toml
      data:
        existingClaim: ${APPNAME}-data
        globalMounts:
          - subPath: data
            path: /data
      nfs:
        type: nfs
        server: "${IP_TRUENAS:=127.0.0.1}"
        path: "${PATH_NAS_PERSIST_K8S:=/home}"
        globalMounts:
          - subPath: ${APPNAME}
            path: /nfs
      tmp:
        type: emptyDir
        medium: Memory
        sizeLimit: 16Mi
        globalMounts:
          - subPath: tmp
            path: /tmp
      tls:
        type: secret
        name: ${APPNAME}-tls
        defaultMode: 0400
        advancedMounts:
          ${APPNAME}:
            main:
              - subPath: tls.crt
                path: /tls/fullchain.pem
                readOnly: true
              - subPath: tls.key
                path: /tls/privkey.pem
                readOnly: true
    configMaps:
      config:
        data:
          server.toml: |
            domain = "${APP_DNS_APPNAME}"
            origin = "https://${APP_DNS_APPNAME}"
            tls_chain = "/tls/fullchain.pem"
            tls_key = "/tls/privkey.pem"
            role = "WriteReplica"
            log_level = "verbose"
            bindaddress = "[::]:8443"
            ldapbindaddress = "[::]:3636"
            trust_x_forward_for = true
            db_path = "/data/${APPNAME}.db"
            db_fs_type = "other"
            [online_backup]
            path = "/backup/"
            schedule = "0 0 22 * * * *"
            versions = 7
    defaultPodOptions:
      automountServiceAccountToken: false
      enableServiceLinks: false
      hostAliases:
        - ip: "${APP_IP_AUTHENTIK:=127.0.0.1}"
          hostnames: ["${APP_DNS_AUTHENTIK:=authentik}"]
      dnsConfig:
        options:
          - name: ndots
            value: "1"
      # hostUsers: true
      # securityContext:
      #   runAsNonRoot: true
      #   runAsUser: &uid ${APP_UID_APPNAME:=1000}
      hostUsers: false
      securityContext:
        runAsNonRoot: true
        runAsUser: &uid 1000
        runAsGroup: *uid
        fsGroup: *uid
        fsGroupChangePolicy: Always
        seccompProfile: { type: "RuntimeDefault" }
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: "{{ .Release.Name }}"
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: "fuckoff.home.arpa/{{ .Release.Name }}"
                    operator: DoesNotExist
    networkpolicies:
      same-ns:
        # either
        controller: app
        # or
        podSelector: {}
        # end
        policyTypes: [Ingress, Egress]
        rules:
          ingress: [from: [{podSelector: {}}]]
          egress: [to: [{podSelector: {}}]]
