---
# yaml-language-server: $schema=https://raw.githubusercontent.com/bjw-s/helm-charts/app-template-3.7.1/charts/other/app-template/schemas/helmrelease-helm-v2beta2.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app mlc-llm
  namespace: *app
spec:
  interval: 5m
  chart:
    spec:
      chart: app-template
      version: 3.7.3
      sourceRef:
        name: bjw-s
        kind: HelmRepository
        namespace: flux-system
  values:
    controllers:
      llama3: &deploy
        type: deployment
        replicas: 1
        strategy: RollingUpdate
        pod:
          labels:
            ingress.home.arpa/nginx-internal: allow
        containers:
          main: &mlc
            image: &img
              repository: jank.ing/jjgadgets/mlc-llm
              tag: 0.19.0@sha256:0c7428cdbf8435ca10e52b4ca564278e48c24ec639c017260cdd39baa3a95384
            args: ["HF://mlc-ai/$(MODEL)", "--enable-debug", "--overrides", "max_num_sequence=1;max_total_seq_length=$(CONTEXT_SIZE)"]
            env: &envMain
              TZ: "${CONFIG_TZ}"
              MLC_JIT_POLICY: "ON"
              MLC_DOWNLOAD_CACHE_POLICY: "READONLY"
              MODEL: &llama3-model "Llama-3.2-3B-Instruct-q4f16_1-MLC"
              CONTEXT_SIZE: "32768" # smaller KV cache needed thus larger size
            securityContext: &sc
              readOnlyRootFilesystem: true
              allowPrivilegeEscalation: false
              capabilities:
                drop: ["ALL"]
            resources: &resources
              requests:
                cpu: "10m"
              limits:
                cpu: "1000m"
                memory: "7.5Gi"
                gpu.intel.com/i915: "1"
            probes:
              liveness:
                enabled: true
              readiness:
                enabled: true
              startup:
                enabled: true
                custom: true
                spec:
                  periodSeconds: 2
                  failureThreshold: 300
                  tcpSocket:
                    port: 8080
      llama3-pull: &job
        type: job
        pod:
          labels:
            egress.home.arpa/internet: allow
        containers:
          main: &pull
            image: *img
            command: ["tini", "-g", "--", "/bin/bash", "-c"]
            args: ["rm -rf /tmp/* && echo '/exit' | mlc_llm chat HF://mlc-ai/$(MODEL) || true; rm -rf /tmp/*"]
            env: &envPull
              TZ: "${CONFIG_TZ}"
              MLC_JIT_POLICY: "OFF" # do on runtime
              MLC_DOWNLOAD_CACHE_POLICY: "ON"
              MODEL: *llama3-model
            securityContext: *sc
            resources:
              requests:
                cpu: "10m"
              limits:
                cpu: "1"
                memory: "10Gi"
      # codellama:
      #   <<: *deploy
      #   containers:
      #     main:
      #       <<: *mlc
      #       env:
      #         <<: *envMain
      #         MODEL: &codellama-model "CodeLlama-7b-hf-q4f32_1-MLC"
      #       resources:
      #         requests:
      #           cpu: "10m"
      #         limits:
      #           cpu: "1000m"
      #           memory: "12Gi"
      #           gpu.intel.com/i915: "1"
      # codellama-pull:
      #   <<: *job
      #   containers:
      #     main:
      #       <<: *pull
      #       env:
      #         <<: *envPull
      #         MODEL: *codellama-model
      phi3:
        <<: *deploy
        containers:
          main:
            <<: *mlc
            env:
              <<: *envMain
              MODEL: &phi3-model "Phi-3.5-mini-instruct-q4f16_1-MLC"
              CONTEXT_SIZE: "16384"
            resources:
              requests:
                cpu: "10m"
                memory: "3Gi"
              limits:
                cpu: "1000m"
                memory: "9Gi" # 2GB params, 0.4GB tmp, rest KV cache
                gpu.intel.com/i915: "1"
      phi3-pull:
        <<: *job
        containers:
          main:
            <<: *pull
            env:
              <<: *envPull
              MODEL: *phi3-model
      qwq:
        <<: *deploy
        containers:
          main:
            <<: *mlc
            args: ["HF://mlc-ai/$(MODEL)", "--enable-debug", "--overrides", "max_num_sequence=1;max_total_seq_length=$(CONTEXT_SIZE)", "--speculative-mode", "$(SPECULATIVE_DECODING)", "--additional-models", "HF://mlc-ai/$(SPECULATIVE_DECODING_MODEL)"]
            env:
              <<: *envMain
              MODEL: &qwq-model "QwQ-32B-q4f16_1-MLC"
              SPECULATIVE_DECODING: "small_draft"
              SPECULATIVE_DECODING_MODEL: "Qwen2.5-0.5B-Instruct-q0f16-MLC"
            resources:
              requests:
                cpu: "10m"
              limits:
                cpu: "1000m"
                memory: "30Gi"
                gpu.intel.com/i915: "1"
      qwq-pull:
        <<: *job
        containers:
          main:
            <<: *pull
            env:
              <<: *envPull
              MODEL: *qwq-model
      # r1-qwen-32b:
      #   <<: *deploy
      #   containers:
      #     main:
      #       <<: *mlc
      #       env:
      #         <<: *envMain
      #         MODEL: &r1-qwen-32b-model "DeepSeek-R1-Distill-Qwen-32B-q4f16_1-MLC"
      #       resources:
      #         requests:
      #           cpu: "10m"
      #         limits:
      #           cpu: "1000m"
      #           memory: "40Gi"
      #           gpu.intel.com/i915: "1"
      # r1-qwen-32b-pull:
      #   <<: *job
      #   containers:
      #     main:
      #       <<: *pull
      #       env:
      #         <<: *envPull
      #         MODEL: *r1-qwen-32b-model
    service:
      llama3: &svc
        controller: llama3
        ports:
          http:
            port: 8080
            protocol: HTTP
            appProtocol: http
      # codellama:
      #   <<: *svc
      #   controller: codellama
      phi3:
        <<: *svc
        controller: phi3
      qwq:
        <<: *svc
        controller: qwq
      # r1-qwen-32b:
      #   <<: *svc
      #   controller: r1-qwen-32b
    ingress:
      llama3:
        className: nginx-internal
        hosts:
          - host: &host "llama3.${DNS_SHORT}"
            paths: &paths
              - path: /
                pathType: Prefix
                service:
                  identifier: llama3
                  port: http
        tls:
          - hosts: [*host]
      # codellama:
      #   className: nginx-internal
      #   hosts:
      #     - host: &host "codellama.${DNS_SHORT}"
      #       paths: &paths
      #         - path: /
      #           pathType: Prefix
      #           service:
      #             identifier: codellama
      #             port: http
      #   tls:
      #     - hosts: [*host]
      phi3:
        className: nginx-internal
        hosts:
          - host: &host "phi3.${DNS_SHORT}"
            paths: &paths
              - path: /
                pathType: Prefix
                service:
                  identifier: phi3
                  port: http
        tls:
          - hosts: [*host]
      qwq:
        className: nginx-internal
        hosts:
          - host: &host "qwq.${DNS_SHORT}"
            paths: &paths
              - path: /
                pathType: Prefix
                service:
                  identifier: qwq
                  port: http
        tls:
          - hosts: [*host]
      # r1-qwen-32b:
      #   className: nginx-internal
      #   hosts:
      #     - host: &host "deepseek.${DNS_SHORT}"
      #       paths: &paths
      #         - path: /
      #           pathType: Prefix
      #           service:
      #             identifier: r1-qwen-32b
      #             port: http
      #   tls:
      #     - hosts: [*host]
    persistence:
      misc:
        existingClaim: mlc-llm-misc
        globalMounts:
          - subPath: data
            path: /data
          - subPath: tmp
            path: /tmp # TODO: check why /tmp on CephFS breaks Git clone
    defaultPodOptions:
      automountServiceAccountToken: false
      enableServiceLinks: false
      hostAliases:
        - ip: "${APP_IP_AUTHENTIK:=127.0.0.1}"
          hostnames: ["${APP_DNS_AUTHENTIK:=authentik}"]
      hostUsers: true
      securityContext:
        runAsNonRoot: true
        runAsUser: &uid ${APP_UID_MLC_LLM:=1000}
        runAsGroup: *uid
        fsGroup: *uid
        fsGroupChangePolicy: Always
        supplementalGroups: [44] # iGPU
        seccompProfile: { type: "Unconfined" } # GPU hangs with RuntimeDefault
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: *app
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: fuckoff.home.arpa/{{ .Release.Name }}
                    operator: DoesNotExist
    networkpolicies:
      from-frontends:
        podSelector: {}
        policyTypes: [Ingress, Egress]
        rules:
          ingress:
            - from:
                - namespaceSelector:
                    matchLabels:
                      kubernetes.io/metadata.name: open-webui
                - namespaceSelector:
                    matchLabels:
                      kubernetes.io/metadata.name: sillytavern
                - namespaceSelector:
                    matchLabels:
                      kubernetes.io/metadata.name: code-server
